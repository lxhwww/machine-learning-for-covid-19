{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ex1_code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lxhwww/machine-learning-for-covid-19/blob/main/ex1_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDIJlbHj4Tqm",
        "outputId": "33e65c1e-ee3e-420b-b6b2-b5a0ae986894"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import&data"
      ],
      "metadata": {
        "id": "_Wkif8EHBNQo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1xs3hgvleo8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6590c5d-0b0f-4a6e-bc31-cb0d7395aa69"
      },
      "source": [
        "#Imports\n",
        "#import xenaPython as xena\n",
        "from statistics import median, mean\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "import pandas as pd \n",
        "from sklearn.impute import SimpleImputer \n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from imblearn.ensemble import BalancedBaggingClassifier, RUSBoostClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, StratifiedKFold\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from pylab import rcParams\n",
        "from scipy.stats import f_oneway\n",
        "from scipy.stats import ttest_ind\n",
        "from collections import Counter\n",
        "import re\n",
        "from keras.utils import np_utils\n",
        "print( \"--- Imports Successful ---\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Imports Successful ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "zvnOfSPn4ntH",
        "outputId": "97f8271f-f2d4-4ab8-c6b3-27d9326cf2c3"
      },
      "source": [
        "#read data\n",
        "df_chunk = pd.read_csv('/content/drive/MyDrive/GSE174818_matrix_processed.csv', chunksize=100000, iterator = True) #使用chunksize分块读取大型csv文件，这里每次读取chunksize为100万\n",
        "\n",
        "chunk_list = [] \n",
        "\n",
        "for chunk in df_chunk:\n",
        "    chunk_list.append(chunk)\n",
        "\n",
        "df_data = pd.concat(chunk_list) #再把这些块组合成一个DataFrame\n",
        "print(df_data.shape)\n",
        "df_data.head(10)\n",
        "\n",
        "#delete pval\n",
        "cols=df_data.columns\n",
        "cols=list(cols)\n",
        "\n",
        "y_label=[]\n",
        "x_data=[]\n",
        "\n",
        "for i in range(len(cols)):\n",
        "  if i%2==1:\n",
        "    df_data.drop([cols[i]],axis=1,inplace=True)\n",
        "  \n",
        "print(df_data.shape)\n",
        "df_data.head(10)\n",
        "\n",
        "#fillna\n",
        "df_data.fillna(value=0,inplace=True)\n",
        "df_data.isnull().sum()\n",
        "\n",
        "\n",
        "#generate\n",
        "reg=r\"N\"\n",
        "y_label=[]\n",
        "x_data=[]\n",
        "x_covid=[]\n",
        "\n",
        "cols_data=df_data.columns\n",
        "cols_data=list(cols_data)\n",
        "\n",
        "count=0\n",
        "for j in range(len(cols_data)):\n",
        "  if re.search(reg,cols_data[j])==None:\n",
        "    y_label.append(1)\n",
        "    count+=1\n",
        "    x_covid.append(list(df_data.iloc[:,j]))\n",
        "  else:\n",
        "    y_label.append(0)\n",
        "  x_data.append(list(df_data.iloc[:,j]))\n",
        "\n",
        "print(y_label)\n",
        "print(np.asarray(x_data).shape)\n",
        "print(count)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-c0c59350a226>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mchunk_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mchunk_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mget_chunk\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m   1072\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m             \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_currow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1074\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXeDYycw6A64",
        "outputId": "ab8058b6-058d-410f-f473-13eccc0be81e"
      },
      "source": [
        "#to array\n",
        "y_label=np.asarray(y_label)\n",
        "x_data=np.asarray(x_data)\n",
        "print(y_label.shape,x_data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(128,) (128, 865859)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06iRIYkApJ-4"
      },
      "source": [
        "df_data.rename(columns={'C100.1':'C10'},inplace=True)#修改列名\n",
        "\n",
        "df_sev = pd.read_csv('/content/drive/MyDrive/IndividualCovidGRAM.csv')\n",
        "df_sev.columns\n",
        "df_sev.loc[(df_sev['Albany_sampleID']=='C21 and C54'),'Albany_sampleID']='C21'#修改列名"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHpDR_tDusCB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a5b7998-c63f-4182-d3fe-88eb6b7bc236"
      },
      "source": [
        "s1=df_data.columns.tolist()\n",
        "s2=df_sev['Albany_sampleID'].tolist()\n",
        "s1, s2 = set(s1), set(s2)\n",
        "s2-(s2&s1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "set()"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkERgM9vz0B5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf377df5-a893-4ca2-8d99-85820a81b5b8"
      },
      "source": [
        "t=[x for x in s1 if x in s2]\n",
        "x_covid=[]\n",
        "y_sev=[]\n",
        "for x in t:\n",
        "  x_covid.append(list(df_data[x]))\n",
        "  y_sev.append(list(df_sev[(df_sev['Albany_sampleID']==x)]['Covid-GRAM Risk Group']))\n",
        "\n",
        "\n",
        "x_covid=np.asarray(x_covid)\n",
        "y_sev=np.asarray(y_sev)\n",
        "\n",
        "print(x_covid.shape)\n",
        "print(y_sev.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(102, 865859)\n",
            "(102, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "menjCMKB1UoB",
        "outputId": "09e5947e-1cd0-4372-89f8-f6ee7852b6f2"
      },
      "source": [
        "#标签化severity\n",
        "from sklearn import preprocessing\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(y_sev.tolist())\n",
        "# transform 以后，这一列数就变成了 [0,  n-1] 这个区间的数，即是  le.classes_ 中的索引\n",
        "y_sev_label=le.transform(y_sev.tolist())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:98: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# read RNA"
      ],
      "metadata": {
        "id": "G8ibjVFV_Pk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#读rns数据\n",
        "df_chunk = pd.read_csv('/content/drive/MyDrive/1.csv', chunksize=100000, iterator = True) #使用chunksize分块读取大型csv文件，这里每次读取chunksize为100万\n",
        "\n",
        "chunk_list = [] \n",
        "\n",
        "for chunk in df_chunk:\n",
        "    chunk_list.append(chunk)\n",
        "\n",
        "df_data_RNA = pd.concat(chunk_list) #再把这些块组合成一个DataFrame\n",
        "print(df_data_RNA.shape)\n",
        "df_data_RNA.head(10)\n",
        "\n",
        "\n",
        "#delete pval\n",
        "cols=df_data_RNA.columns\n",
        "cols=list(cols)\n",
        "\n",
        "\n",
        "#fillna\n",
        "df_data_RNA.fillna(value=0,inplace=True)\n",
        "df_data_RNA.isnull().sum()\n",
        "\n",
        "\n",
        "df_data_RNA.head(10)\n"
      ],
      "metadata": {
        "id": "dq01I8TzGusR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "outputId": "cfebbaad-298a-41b6-e2f8-a62e4f64b92c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(30185, 54)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       507-V     1189-V     1406-V     1918-V     1951-V     1995-V  \\\n",
              "0  14.070559  13.798071  13.581000  13.501768  13.439851  12.903939   \n",
              "1  13.928768  13.768303  11.265803  14.073185  12.488195  13.381280   \n",
              "2  13.949530  12.926018  13.130841  12.218066  12.748996  13.277281   \n",
              "3  13.180418  13.028500  12.681845  12.038974  10.743518  13.604989   \n",
              "4  16.122646  14.470176  14.723095  13.906649  13.461510  15.058014   \n",
              "5  13.849207  10.790313  12.664782  13.112241   9.205350  10.790313   \n",
              "6  11.701272  10.322760  10.808187  12.088295  10.322760  10.322760   \n",
              "7  11.854753  11.854753  11.854753  11.854753  11.854753  13.439715   \n",
              "8  13.632979  13.649695  13.258536  13.503150  13.429112  13.623898   \n",
              "9  13.604221  13.868548  13.389481  13.483627  13.322409  13.270727   \n",
              "\n",
              "      2064-V     3054-V     3136-V     3538-V  ...         81         82  \\\n",
              "0  13.287856  13.033640  13.900911  13.333453  ...  12.779625  12.960987   \n",
              "1  12.850765  13.140272  13.381280   9.680840  ...  13.382329  14.435749   \n",
              "2  13.024633  12.918938  12.530818  12.748998  ...  12.045388  12.798751   \n",
              "3  11.393788  12.644491  13.262656  13.972909  ...  13.359914  14.055795   \n",
              "4  13.715828  13.931961  14.693802  13.594265  ...  11.173934  11.730327   \n",
              "5  12.664782   9.205350  12.375275   9.205350  ...  15.834707  14.063331   \n",
              "6   9.585794   9.585794  12.088295   9.585794  ...  13.358384  11.170757   \n",
              "7  13.439715  13.439715  13.439715  15.314185  ...  14.176681  16.102680   \n",
              "8  13.653525  13.261884  13.411125  13.597630  ...  12.417896  13.454216   \n",
              "9  13.652166  13.297420  13.489394  13.597905  ...  12.475918  13.504521   \n",
              "\n",
              "          83         84         85         87         91         92  \\\n",
              "0  13.511849  13.884385  13.723434  14.082154  12.701183  13.171894   \n",
              "1  12.002780  14.436251  13.928789  14.725444  13.381280  13.768794   \n",
              "2  13.761597  13.105930  13.761598  12.588533  13.904791  12.342374   \n",
              "3  14.333685  14.533926  13.982778  14.357402  13.258881  13.355466   \n",
              "4  13.017207  14.678928  13.017207  14.163286  12.512734  10.786910   \n",
              "5  13.960238  15.545200  15.681084  15.681084  15.136088  12.664782   \n",
              "6   8.000832  11.170757   8.000832   8.000832   8.000832   8.000832   \n",
              "7  11.854753  15.555193  13.439715  11.854753  11.854753  11.854753   \n",
              "8  13.657344  13.483201  13.787372  13.468782  14.302699  12.779800   \n",
              "9  13.873533  13.805276  13.936141  13.872077  13.991574  13.181835   \n",
              "\n",
              "          95        101  \n",
              "0  13.335847  13.305619  \n",
              "1  13.140545  13.768303  \n",
              "2  11.559963  12.273657  \n",
              "3  13.581636  13.926458  \n",
              "4  12.040665  12.578321  \n",
              "5  14.631615  12.905790  \n",
              "6   8.000832  12.755719  \n",
              "7  11.854753  16.247070  \n",
              "8  13.063133  13.271888  \n",
              "9  12.890172  13.287888  \n",
              "\n",
              "[10 rows x 54 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5bb39ddd-35a8-4399-aa52-1445f2eb5153\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>507-V</th>\n",
              "      <th>1189-V</th>\n",
              "      <th>1406-V</th>\n",
              "      <th>1918-V</th>\n",
              "      <th>1951-V</th>\n",
              "      <th>1995-V</th>\n",
              "      <th>2064-V</th>\n",
              "      <th>3054-V</th>\n",
              "      <th>3136-V</th>\n",
              "      <th>3538-V</th>\n",
              "      <th>...</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>87</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>95</th>\n",
              "      <th>101</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>14.070559</td>\n",
              "      <td>13.798071</td>\n",
              "      <td>13.581000</td>\n",
              "      <td>13.501768</td>\n",
              "      <td>13.439851</td>\n",
              "      <td>12.903939</td>\n",
              "      <td>13.287856</td>\n",
              "      <td>13.033640</td>\n",
              "      <td>13.900911</td>\n",
              "      <td>13.333453</td>\n",
              "      <td>...</td>\n",
              "      <td>12.779625</td>\n",
              "      <td>12.960987</td>\n",
              "      <td>13.511849</td>\n",
              "      <td>13.884385</td>\n",
              "      <td>13.723434</td>\n",
              "      <td>14.082154</td>\n",
              "      <td>12.701183</td>\n",
              "      <td>13.171894</td>\n",
              "      <td>13.335847</td>\n",
              "      <td>13.305619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>13.928768</td>\n",
              "      <td>13.768303</td>\n",
              "      <td>11.265803</td>\n",
              "      <td>14.073185</td>\n",
              "      <td>12.488195</td>\n",
              "      <td>13.381280</td>\n",
              "      <td>12.850765</td>\n",
              "      <td>13.140272</td>\n",
              "      <td>13.381280</td>\n",
              "      <td>9.680840</td>\n",
              "      <td>...</td>\n",
              "      <td>13.382329</td>\n",
              "      <td>14.435749</td>\n",
              "      <td>12.002780</td>\n",
              "      <td>14.436251</td>\n",
              "      <td>13.928789</td>\n",
              "      <td>14.725444</td>\n",
              "      <td>13.381280</td>\n",
              "      <td>13.768794</td>\n",
              "      <td>13.140545</td>\n",
              "      <td>13.768303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>13.949530</td>\n",
              "      <td>12.926018</td>\n",
              "      <td>13.130841</td>\n",
              "      <td>12.218066</td>\n",
              "      <td>12.748996</td>\n",
              "      <td>13.277281</td>\n",
              "      <td>13.024633</td>\n",
              "      <td>12.918938</td>\n",
              "      <td>12.530818</td>\n",
              "      <td>12.748998</td>\n",
              "      <td>...</td>\n",
              "      <td>12.045388</td>\n",
              "      <td>12.798751</td>\n",
              "      <td>13.761597</td>\n",
              "      <td>13.105930</td>\n",
              "      <td>13.761598</td>\n",
              "      <td>12.588533</td>\n",
              "      <td>13.904791</td>\n",
              "      <td>12.342374</td>\n",
              "      <td>11.559963</td>\n",
              "      <td>12.273657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>13.180418</td>\n",
              "      <td>13.028500</td>\n",
              "      <td>12.681845</td>\n",
              "      <td>12.038974</td>\n",
              "      <td>10.743518</td>\n",
              "      <td>13.604989</td>\n",
              "      <td>11.393788</td>\n",
              "      <td>12.644491</td>\n",
              "      <td>13.262656</td>\n",
              "      <td>13.972909</td>\n",
              "      <td>...</td>\n",
              "      <td>13.359914</td>\n",
              "      <td>14.055795</td>\n",
              "      <td>14.333685</td>\n",
              "      <td>14.533926</td>\n",
              "      <td>13.982778</td>\n",
              "      <td>14.357402</td>\n",
              "      <td>13.258881</td>\n",
              "      <td>13.355466</td>\n",
              "      <td>13.581636</td>\n",
              "      <td>13.926458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>16.122646</td>\n",
              "      <td>14.470176</td>\n",
              "      <td>14.723095</td>\n",
              "      <td>13.906649</td>\n",
              "      <td>13.461510</td>\n",
              "      <td>15.058014</td>\n",
              "      <td>13.715828</td>\n",
              "      <td>13.931961</td>\n",
              "      <td>14.693802</td>\n",
              "      <td>13.594265</td>\n",
              "      <td>...</td>\n",
              "      <td>11.173934</td>\n",
              "      <td>11.730327</td>\n",
              "      <td>13.017207</td>\n",
              "      <td>14.678928</td>\n",
              "      <td>13.017207</td>\n",
              "      <td>14.163286</td>\n",
              "      <td>12.512734</td>\n",
              "      <td>10.786910</td>\n",
              "      <td>12.040665</td>\n",
              "      <td>12.578321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>13.849207</td>\n",
              "      <td>10.790313</td>\n",
              "      <td>12.664782</td>\n",
              "      <td>13.112241</td>\n",
              "      <td>9.205350</td>\n",
              "      <td>10.790313</td>\n",
              "      <td>12.664782</td>\n",
              "      <td>9.205350</td>\n",
              "      <td>12.375275</td>\n",
              "      <td>9.205350</td>\n",
              "      <td>...</td>\n",
              "      <td>15.834707</td>\n",
              "      <td>14.063331</td>\n",
              "      <td>13.960238</td>\n",
              "      <td>15.545200</td>\n",
              "      <td>15.681084</td>\n",
              "      <td>15.681084</td>\n",
              "      <td>15.136088</td>\n",
              "      <td>12.664782</td>\n",
              "      <td>14.631615</td>\n",
              "      <td>12.905790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>11.701272</td>\n",
              "      <td>10.322760</td>\n",
              "      <td>10.808187</td>\n",
              "      <td>12.088295</td>\n",
              "      <td>10.322760</td>\n",
              "      <td>10.322760</td>\n",
              "      <td>9.585794</td>\n",
              "      <td>9.585794</td>\n",
              "      <td>12.088295</td>\n",
              "      <td>9.585794</td>\n",
              "      <td>...</td>\n",
              "      <td>13.358384</td>\n",
              "      <td>11.170757</td>\n",
              "      <td>8.000832</td>\n",
              "      <td>11.170757</td>\n",
              "      <td>8.000832</td>\n",
              "      <td>8.000832</td>\n",
              "      <td>8.000832</td>\n",
              "      <td>8.000832</td>\n",
              "      <td>8.000832</td>\n",
              "      <td>12.755719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>11.854753</td>\n",
              "      <td>11.854753</td>\n",
              "      <td>11.854753</td>\n",
              "      <td>11.854753</td>\n",
              "      <td>11.854753</td>\n",
              "      <td>13.439715</td>\n",
              "      <td>13.439715</td>\n",
              "      <td>13.439715</td>\n",
              "      <td>13.439715</td>\n",
              "      <td>15.314185</td>\n",
              "      <td>...</td>\n",
              "      <td>14.176681</td>\n",
              "      <td>16.102680</td>\n",
              "      <td>11.854753</td>\n",
              "      <td>15.555193</td>\n",
              "      <td>13.439715</td>\n",
              "      <td>11.854753</td>\n",
              "      <td>11.854753</td>\n",
              "      <td>11.854753</td>\n",
              "      <td>11.854753</td>\n",
              "      <td>16.247070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>13.632979</td>\n",
              "      <td>13.649695</td>\n",
              "      <td>13.258536</td>\n",
              "      <td>13.503150</td>\n",
              "      <td>13.429112</td>\n",
              "      <td>13.623898</td>\n",
              "      <td>13.653525</td>\n",
              "      <td>13.261884</td>\n",
              "      <td>13.411125</td>\n",
              "      <td>13.597630</td>\n",
              "      <td>...</td>\n",
              "      <td>12.417896</td>\n",
              "      <td>13.454216</td>\n",
              "      <td>13.657344</td>\n",
              "      <td>13.483201</td>\n",
              "      <td>13.787372</td>\n",
              "      <td>13.468782</td>\n",
              "      <td>14.302699</td>\n",
              "      <td>12.779800</td>\n",
              "      <td>13.063133</td>\n",
              "      <td>13.271888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>13.604221</td>\n",
              "      <td>13.868548</td>\n",
              "      <td>13.389481</td>\n",
              "      <td>13.483627</td>\n",
              "      <td>13.322409</td>\n",
              "      <td>13.270727</td>\n",
              "      <td>13.652166</td>\n",
              "      <td>13.297420</td>\n",
              "      <td>13.489394</td>\n",
              "      <td>13.597905</td>\n",
              "      <td>...</td>\n",
              "      <td>12.475918</td>\n",
              "      <td>13.504521</td>\n",
              "      <td>13.873533</td>\n",
              "      <td>13.805276</td>\n",
              "      <td>13.936141</td>\n",
              "      <td>13.872077</td>\n",
              "      <td>13.991574</td>\n",
              "      <td>13.181835</td>\n",
              "      <td>12.890172</td>\n",
              "      <td>13.287888</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 54 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5bb39ddd-35a8-4399-aa52-1445f2eb5153')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5bb39ddd-35a8-4399-aa52-1445f2eb5153 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5bb39ddd-35a8-4399-aa52-1445f2eb5153');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#generate\n",
        "y_label_RNA=[]\n",
        "x_data_RNA=[]\n",
        "\n",
        "\n",
        "reg=r\"V\"\n",
        "y_label_RNA=[]\n",
        "x_data_RNA=[]\n",
        "\n",
        "cols_data_RNA=df_data_RNA.columns\n",
        "cols_data_RNA=list(cols_data_RNA)\n",
        "\n",
        "count=0\n",
        "for j in range(len(cols_data_RNA)):\n",
        "  if re.search(reg,cols_data_RNA[j])==None:\n",
        "    y_label_RNA.append(1)\n",
        "    count+=1\n",
        "    #x_covid.append(list(df_data.iloc[:,j]))\n",
        "  else:\n",
        "    y_label_RNA.append(0)\n",
        "  x_data_RNA.append(list(df_data_RNA.iloc[:,j]))\n",
        "\n",
        "print(y_label_RNA)\n",
        "x_data_RNA=np.asarray(x_data_RNA)\n",
        "print(np.asarray(x_data_RNA).shape)\n",
        "print(count)"
      ],
      "metadata": {
        "id": "GP-ZLcdw-9HO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbdc9e59-7b2d-458f-c186-69990996413c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "(54, 30185)\n",
            "44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Protein"
      ],
      "metadata": {
        "id": "hVnHQtZeJE-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_pm = pd.read_csv('/content/drive/MyDrive/Prot_and_meta_matrix.csv') \n",
        "print(df_pm.shape)\n",
        "df_pm.head(1639)\n",
        "\n",
        "df_pm.fillna(value=0,inplace=True)"
      ],
      "metadata": {
        "id": "6zUg194tJK-_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a361fe7a-09d3-4f27-e881-31b8c945ea18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1639, 31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_label_pm=[]\n",
        "x_data_pm=[]\n",
        "\n",
        "y_label_pm=[]\n",
        "x_data_pm=[]\n",
        "\n",
        "cols_data_pm=df_pm.columns\n",
        "cols_data_pm=list(cols_data_pm)\n",
        "\n",
        "count=0\n",
        "for j in range(len(cols_data_pm)):\n",
        "  y_label_pm.append(list(df_pm.iloc[1638:,j]))\n",
        "  x_data_pm.append(list(df_pm.iloc[:1638,j]))\n",
        "\n",
        "print(y_label_pm)\n",
        "y_label_pm=np.asarray(y_label_pm)\n",
        "x_data_pm=np.asarray(x_data_pm)\n",
        "print(np.asarray(x_data_pm).shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itoMwM4jfyWB",
        "outputId": "965a47b8-8c9b-4153-88f8-f42f1e8b6fe1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0]]\n",
            "(31, 1638)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_pm_c2 = pd.read_csv('/content/drive/MyDrive/Prot_and_meta_matrix_C2.csv') \n",
        "print(df_pm_c2.shape)\n",
        "df_pm_c2.head()\n",
        "\n",
        "df_pm_c2.fillna(value=0,inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hO816G-aozCn",
        "outputId": "5c911430-a2e4-4153-964c-fba78859da82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1590, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_label_pm_c2=[]\n",
        "x_data_pm_c2=[]\n",
        "\n",
        "cols_data_pm_c2=df_pm_c2.columns\n",
        "cols_data_pm_c2=list(cols_data_pm_c2)\n",
        "\n",
        "count=0\n",
        "for j in range(len(cols_data_pm_c2)):\n",
        "  y_label_pm_c2.append(list(df_pm_c2.iloc[1589:,j]))\n",
        "  x_data_pm_c2.append(list(df_pm_c2.iloc[:1589,j]))\n",
        "\n",
        "print(y_label_pm_c2)\n",
        "y_label_pm_c2=np.asarray(y_label_pm_c2)\n",
        "x_data_pm_c2=np.asarray(x_data_pm_c2)\n",
        "print(np.asarray(x_data_pm_c2).shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-d_5mWVCpMXT",
        "outputId": "54604927-a4ed-40ac-8e59-813ebd49d669"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0]]\n",
            "(10, 1589)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_data_proteins_test= pd.read_csv('/content/drive/MyDrive/Proteomics_proteins_test_CSV.csv') #使用chunksize分块读取大型csv文件，这里每次读取chunksize为100万\n",
        "\n",
        "print(df_data_proteins_test.shape)\n",
        "\n",
        "\n",
        "for i in range(23,30):\n",
        "    df_data_proteins_test.drop([cols[i]],inplace=True)\n",
        "\n",
        "df_data_proteins_test.head(50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 749
        },
        "id": "fLHBzzPf1IMT",
        "outputId": "d78eb3a1-5cb3-4840-a5b3-fd4b8d6ef011"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1638, 31)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-71b28ef187df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m23\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mdf_data_proteins_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mdf_data_proteins_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4172\u001b[0m             \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4173\u001b[0m             \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4174\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4175\u001b[0m         )\n\u001b[1;32m   4176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3887\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3888\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3889\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3891\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   3921\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3922\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3923\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3924\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   5285\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5286\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5287\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5288\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5289\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['F3_126'] not found in axis\""
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PCA"
      ],
      "metadata": {
        "id": "9Mf18GkrBWh6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWVQMwxzDnfq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "a89f04c0-8403-4473-e2fb-2470a1ff09e8"
      },
      "source": [
        "#feature extraction PCA,for severity\n",
        "from sklearn.decomposition import PCA  \n",
        "from pandas.core.frame import DataFrame\n",
        "\n",
        "pca_sk = PCA(n_components=100)\n",
        "newMat_sev = pca_sk.fit_transform(x_covid)  \n",
        "\n",
        "data2 = DataFrame(newMat_sev)\n",
        "data2.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-4faa0ff37d42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpca_sk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mnewMat_sev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca_sk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_covid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdata2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewMat_sev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'x_covid' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#feature extraction PCA,for severity\n",
        "from sklearn.decomposition import PCA  \n",
        "from pandas.core.frame import DataFrame\n",
        "\n",
        "pca_sk = PCA(n_components=50)\n",
        "newMat_RNA = pca_sk.fit_transform(x_data_RNA)  \n",
        "\n",
        "data_3 = DataFrame(newMat_RNA)\n",
        "data_3.head(10)\n",
        "data_3.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZBhLm-RWwz-",
        "outputId": "f6e361c1-03b4-45c8-8207-07fb76900784"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(54, 50)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3bsxlFbF9kH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "outputId": "d36d881f-5934-426b-e0f3-8ea40986c89b"
      },
      "source": [
        "#feature extraction PCA\n",
        "from sklearn.decomposition import PCA  \n",
        "from pandas.core.frame import DataFrame\n",
        "\n",
        "pca_sk = PCA(n_components=100)\n",
        "newMat = pca_sk.fit_transform(x_data)  \n",
        "\n",
        "data1 = DataFrame(newMat)\n",
        "data1.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          0          1          2          3         4         5         6   \\\n",
              "0  70.424070   0.027162  -8.611878 -11.296879 -1.576333  9.905973  8.678106   \n",
              "1  21.502568   7.586904 -26.610598  -0.076713 -0.696440 -2.924814 -3.134700   \n",
              "2  27.067539  18.755577   5.471773  -1.512824 -2.092358 -3.525617 -4.624186   \n",
              "3  32.867180  -4.668383 -13.184994   1.957165 -3.194763  2.939815  4.932708   \n",
              "4   2.498947  17.805380   0.042026  -1.539490 -3.107372 -1.783666 -4.114646   \n",
              "5   6.163955  11.477141   7.494331   5.592167 -3.235964 -3.393347 -2.996309   \n",
              "6  30.864834 -23.955449   5.063940   5.776237 -3.788845 -3.842123 -5.898905   \n",
              "7  19.273341 -11.217911 -12.209465   1.384818 -0.657038 -1.123660 -1.940782   \n",
              "8  33.213828  10.953101  14.400820   4.567252 -3.243584 -5.928109 -5.830744   \n",
              "9   1.595732  15.824697   1.247105  -5.528160  1.611676 -2.935285 -4.921876   \n",
              "\n",
              "         7         8         9   ...        90        91        92        93  \\\n",
              "0  4.887482 -1.357985 -6.216125  ... -2.220217  1.828585  1.206632  0.201295   \n",
              "1  1.274345  0.026485 -1.952106  ... -3.140704 -5.189980 -3.677689  0.588202   \n",
              "2  1.295705 -1.186529 -3.365394  ...  1.277135  7.729726  2.783378  3.070216   \n",
              "3  8.012296 -2.162859 -3.062727  ...  1.928474  1.935955 -1.019117 -8.491985   \n",
              "4  6.555326 -0.361649  3.324415  ... -0.080788 -4.483116  0.312291  0.163893   \n",
              "5  4.886870 -1.277777  0.316791  ... -1.071715 -2.753847 -1.861379 -1.718836   \n",
              "6  3.427456  0.235979  1.056285  ... -3.230414  7.003517  2.678313  1.179281   \n",
              "7  5.635521  1.189560  0.722865  ... -0.463278  0.016870  0.918023  0.482697   \n",
              "8 -3.801369 -1.664601 -0.730666  ...  0.502114 -0.803812  0.603394 -1.555515   \n",
              "9  3.973000  1.242139 -1.178363  ... -1.377523 -1.365289 -0.818178 -0.809576   \n",
              "\n",
              "         94        95        96        97        98        99  \n",
              "0 -2.997471  1.968226  1.393618 -1.694209 -1.013953  1.902540  \n",
              "1  0.994783  0.480879  1.910200  1.606191  2.050419 -2.228954  \n",
              "2  6.304978 -1.397202 -6.411205 -1.315237  5.855553  3.586756  \n",
              "3 -1.782772  0.231474 -4.904790  3.800996  0.379552  3.321249  \n",
              "4  0.829209 -2.795150 -2.836590 -0.378488  0.099091  0.225342  \n",
              "5 -1.063619 -1.857000 -3.156151 -0.677290  0.640762 -1.277704  \n",
              "6 -2.456651 -2.782989  5.130361 -4.841263  1.639696 -0.213652  \n",
              "7 -0.181996  2.372150 -0.659196  0.815262 -0.176372 -0.017767  \n",
              "8  0.217666  0.691781  0.535760  1.609902 -1.115356 -0.896995  \n",
              "9  0.664194 -0.854454  2.645029  0.476212  0.316193  0.866320  \n",
              "\n",
              "[10 rows x 100 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-99a5383c-2eb3-4647-aa1b-3e45c8d44982\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>70.424070</td>\n",
              "      <td>0.027162</td>\n",
              "      <td>-8.611878</td>\n",
              "      <td>-11.296879</td>\n",
              "      <td>-1.576333</td>\n",
              "      <td>9.905973</td>\n",
              "      <td>8.678106</td>\n",
              "      <td>4.887482</td>\n",
              "      <td>-1.357985</td>\n",
              "      <td>-6.216125</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.220217</td>\n",
              "      <td>1.828585</td>\n",
              "      <td>1.206632</td>\n",
              "      <td>0.201295</td>\n",
              "      <td>-2.997471</td>\n",
              "      <td>1.968226</td>\n",
              "      <td>1.393618</td>\n",
              "      <td>-1.694209</td>\n",
              "      <td>-1.013953</td>\n",
              "      <td>1.902540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>21.502568</td>\n",
              "      <td>7.586904</td>\n",
              "      <td>-26.610598</td>\n",
              "      <td>-0.076713</td>\n",
              "      <td>-0.696440</td>\n",
              "      <td>-2.924814</td>\n",
              "      <td>-3.134700</td>\n",
              "      <td>1.274345</td>\n",
              "      <td>0.026485</td>\n",
              "      <td>-1.952106</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.140704</td>\n",
              "      <td>-5.189980</td>\n",
              "      <td>-3.677689</td>\n",
              "      <td>0.588202</td>\n",
              "      <td>0.994783</td>\n",
              "      <td>0.480879</td>\n",
              "      <td>1.910200</td>\n",
              "      <td>1.606191</td>\n",
              "      <td>2.050419</td>\n",
              "      <td>-2.228954</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>27.067539</td>\n",
              "      <td>18.755577</td>\n",
              "      <td>5.471773</td>\n",
              "      <td>-1.512824</td>\n",
              "      <td>-2.092358</td>\n",
              "      <td>-3.525617</td>\n",
              "      <td>-4.624186</td>\n",
              "      <td>1.295705</td>\n",
              "      <td>-1.186529</td>\n",
              "      <td>-3.365394</td>\n",
              "      <td>...</td>\n",
              "      <td>1.277135</td>\n",
              "      <td>7.729726</td>\n",
              "      <td>2.783378</td>\n",
              "      <td>3.070216</td>\n",
              "      <td>6.304978</td>\n",
              "      <td>-1.397202</td>\n",
              "      <td>-6.411205</td>\n",
              "      <td>-1.315237</td>\n",
              "      <td>5.855553</td>\n",
              "      <td>3.586756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>32.867180</td>\n",
              "      <td>-4.668383</td>\n",
              "      <td>-13.184994</td>\n",
              "      <td>1.957165</td>\n",
              "      <td>-3.194763</td>\n",
              "      <td>2.939815</td>\n",
              "      <td>4.932708</td>\n",
              "      <td>8.012296</td>\n",
              "      <td>-2.162859</td>\n",
              "      <td>-3.062727</td>\n",
              "      <td>...</td>\n",
              "      <td>1.928474</td>\n",
              "      <td>1.935955</td>\n",
              "      <td>-1.019117</td>\n",
              "      <td>-8.491985</td>\n",
              "      <td>-1.782772</td>\n",
              "      <td>0.231474</td>\n",
              "      <td>-4.904790</td>\n",
              "      <td>3.800996</td>\n",
              "      <td>0.379552</td>\n",
              "      <td>3.321249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.498947</td>\n",
              "      <td>17.805380</td>\n",
              "      <td>0.042026</td>\n",
              "      <td>-1.539490</td>\n",
              "      <td>-3.107372</td>\n",
              "      <td>-1.783666</td>\n",
              "      <td>-4.114646</td>\n",
              "      <td>6.555326</td>\n",
              "      <td>-0.361649</td>\n",
              "      <td>3.324415</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.080788</td>\n",
              "      <td>-4.483116</td>\n",
              "      <td>0.312291</td>\n",
              "      <td>0.163893</td>\n",
              "      <td>0.829209</td>\n",
              "      <td>-2.795150</td>\n",
              "      <td>-2.836590</td>\n",
              "      <td>-0.378488</td>\n",
              "      <td>0.099091</td>\n",
              "      <td>0.225342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6.163955</td>\n",
              "      <td>11.477141</td>\n",
              "      <td>7.494331</td>\n",
              "      <td>5.592167</td>\n",
              "      <td>-3.235964</td>\n",
              "      <td>-3.393347</td>\n",
              "      <td>-2.996309</td>\n",
              "      <td>4.886870</td>\n",
              "      <td>-1.277777</td>\n",
              "      <td>0.316791</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.071715</td>\n",
              "      <td>-2.753847</td>\n",
              "      <td>-1.861379</td>\n",
              "      <td>-1.718836</td>\n",
              "      <td>-1.063619</td>\n",
              "      <td>-1.857000</td>\n",
              "      <td>-3.156151</td>\n",
              "      <td>-0.677290</td>\n",
              "      <td>0.640762</td>\n",
              "      <td>-1.277704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>30.864834</td>\n",
              "      <td>-23.955449</td>\n",
              "      <td>5.063940</td>\n",
              "      <td>5.776237</td>\n",
              "      <td>-3.788845</td>\n",
              "      <td>-3.842123</td>\n",
              "      <td>-5.898905</td>\n",
              "      <td>3.427456</td>\n",
              "      <td>0.235979</td>\n",
              "      <td>1.056285</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.230414</td>\n",
              "      <td>7.003517</td>\n",
              "      <td>2.678313</td>\n",
              "      <td>1.179281</td>\n",
              "      <td>-2.456651</td>\n",
              "      <td>-2.782989</td>\n",
              "      <td>5.130361</td>\n",
              "      <td>-4.841263</td>\n",
              "      <td>1.639696</td>\n",
              "      <td>-0.213652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>19.273341</td>\n",
              "      <td>-11.217911</td>\n",
              "      <td>-12.209465</td>\n",
              "      <td>1.384818</td>\n",
              "      <td>-0.657038</td>\n",
              "      <td>-1.123660</td>\n",
              "      <td>-1.940782</td>\n",
              "      <td>5.635521</td>\n",
              "      <td>1.189560</td>\n",
              "      <td>0.722865</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.463278</td>\n",
              "      <td>0.016870</td>\n",
              "      <td>0.918023</td>\n",
              "      <td>0.482697</td>\n",
              "      <td>-0.181996</td>\n",
              "      <td>2.372150</td>\n",
              "      <td>-0.659196</td>\n",
              "      <td>0.815262</td>\n",
              "      <td>-0.176372</td>\n",
              "      <td>-0.017767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>33.213828</td>\n",
              "      <td>10.953101</td>\n",
              "      <td>14.400820</td>\n",
              "      <td>4.567252</td>\n",
              "      <td>-3.243584</td>\n",
              "      <td>-5.928109</td>\n",
              "      <td>-5.830744</td>\n",
              "      <td>-3.801369</td>\n",
              "      <td>-1.664601</td>\n",
              "      <td>-0.730666</td>\n",
              "      <td>...</td>\n",
              "      <td>0.502114</td>\n",
              "      <td>-0.803812</td>\n",
              "      <td>0.603394</td>\n",
              "      <td>-1.555515</td>\n",
              "      <td>0.217666</td>\n",
              "      <td>0.691781</td>\n",
              "      <td>0.535760</td>\n",
              "      <td>1.609902</td>\n",
              "      <td>-1.115356</td>\n",
              "      <td>-0.896995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1.595732</td>\n",
              "      <td>15.824697</td>\n",
              "      <td>1.247105</td>\n",
              "      <td>-5.528160</td>\n",
              "      <td>1.611676</td>\n",
              "      <td>-2.935285</td>\n",
              "      <td>-4.921876</td>\n",
              "      <td>3.973000</td>\n",
              "      <td>1.242139</td>\n",
              "      <td>-1.178363</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.377523</td>\n",
              "      <td>-1.365289</td>\n",
              "      <td>-0.818178</td>\n",
              "      <td>-0.809576</td>\n",
              "      <td>0.664194</td>\n",
              "      <td>-0.854454</td>\n",
              "      <td>2.645029</td>\n",
              "      <td>0.476212</td>\n",
              "      <td>0.316193</td>\n",
              "      <td>0.866320</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 100 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-99a5383c-2eb3-4647-aa1b-3e45c8d44982')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-99a5383c-2eb3-4647-aa1b-3e45c8d44982 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-99a5383c-2eb3-4647-aa1b-3e45c8d44982');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWdJwAkmflPf",
        "outputId": "e9bf8cd6-e17c-4db2-81fd-71ab5dedacf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(128, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#feature extraction PCA\n",
        "from sklearn.decomposition import PCA  \n",
        "from pandas.core.frame import DataFrame\n",
        "\n",
        "pca_sk = PCA(n_components=10)\n",
        "newMat_pm = pca_sk.fit_transform(x_data_pm)  \n",
        "\n",
        "data1 = DataFrame(newMat_pm)\n",
        "data1.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "xo6oq-wQQbDt",
        "outputId": "3a14b2b8-ae57-40b3-f3d7-2495a217b4b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-7e0588b3-6761-4654-9058-78a2691f6b77\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.506223</td>\n",
              "      <td>4.941293</td>\n",
              "      <td>-12.145561</td>\n",
              "      <td>-12.717628</td>\n",
              "      <td>-8.381940</td>\n",
              "      <td>1.768469</td>\n",
              "      <td>-4.320300</td>\n",
              "      <td>1.458537</td>\n",
              "      <td>-3.641046</td>\n",
              "      <td>12.620717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-6.511002</td>\n",
              "      <td>1.888006</td>\n",
              "      <td>-19.096101</td>\n",
              "      <td>-9.195652</td>\n",
              "      <td>0.770401</td>\n",
              "      <td>-8.833153</td>\n",
              "      <td>0.886853</td>\n",
              "      <td>10.651408</td>\n",
              "      <td>11.980405</td>\n",
              "      <td>-2.048504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-3.281161</td>\n",
              "      <td>-1.488949</td>\n",
              "      <td>9.339293</td>\n",
              "      <td>-1.358326</td>\n",
              "      <td>-0.530648</td>\n",
              "      <td>-6.507844</td>\n",
              "      <td>-6.427221</td>\n",
              "      <td>3.601074</td>\n",
              "      <td>-1.243163</td>\n",
              "      <td>-2.445833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-4.083976</td>\n",
              "      <td>3.832610</td>\n",
              "      <td>11.235773</td>\n",
              "      <td>-2.028257</td>\n",
              "      <td>1.196753</td>\n",
              "      <td>-12.796828</td>\n",
              "      <td>-1.860877</td>\n",
              "      <td>11.188882</td>\n",
              "      <td>0.379290</td>\n",
              "      <td>12.553828</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3.921686</td>\n",
              "      <td>-9.364057</td>\n",
              "      <td>10.836048</td>\n",
              "      <td>-13.261133</td>\n",
              "      <td>0.446717</td>\n",
              "      <td>-6.727777</td>\n",
              "      <td>-3.639084</td>\n",
              "      <td>0.165564</td>\n",
              "      <td>-2.391553</td>\n",
              "      <td>16.111234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>3.699987</td>\n",
              "      <td>-20.871125</td>\n",
              "      <td>-0.121494</td>\n",
              "      <td>-3.604278</td>\n",
              "      <td>-9.346621</td>\n",
              "      <td>-8.595697</td>\n",
              "      <td>-5.064906</td>\n",
              "      <td>-5.670873</td>\n",
              "      <td>13.170855</td>\n",
              "      <td>-9.200022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2.166755</td>\n",
              "      <td>4.721834</td>\n",
              "      <td>12.354983</td>\n",
              "      <td>0.007537</td>\n",
              "      <td>-11.826883</td>\n",
              "      <td>0.381970</td>\n",
              "      <td>-3.641086</td>\n",
              "      <td>-10.216622</td>\n",
              "      <td>2.304482</td>\n",
              "      <td>-7.545882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>62.366266</td>\n",
              "      <td>14.330558</td>\n",
              "      <td>-6.262381</td>\n",
              "      <td>19.162952</td>\n",
              "      <td>3.945281</td>\n",
              "      <td>-4.104920</td>\n",
              "      <td>-7.669346</td>\n",
              "      <td>2.437832</td>\n",
              "      <td>6.260088</td>\n",
              "      <td>4.897240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>-5.523902</td>\n",
              "      <td>38.602066</td>\n",
              "      <td>5.258477</td>\n",
              "      <td>-18.840640</td>\n",
              "      <td>18.801235</td>\n",
              "      <td>3.293863</td>\n",
              "      <td>-3.636519</td>\n",
              "      <td>-7.714512</td>\n",
              "      <td>7.107791</td>\n",
              "      <td>-6.926707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.835655</td>\n",
              "      <td>10.300563</td>\n",
              "      <td>19.887098</td>\n",
              "      <td>2.258253</td>\n",
              "      <td>4.754845</td>\n",
              "      <td>-2.956959</td>\n",
              "      <td>8.829932</td>\n",
              "      <td>4.109268</td>\n",
              "      <td>-4.517253</td>\n",
              "      <td>-6.286026</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7e0588b3-6761-4654-9058-78a2691f6b77')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7e0588b3-6761-4654-9058-78a2691f6b77 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7e0588b3-6761-4654-9058-78a2691f6b77');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           0          1          2  ...          7          8          9\n",
              "0  -1.506223   4.941293 -12.145561  ...   1.458537  -3.641046  12.620717\n",
              "1  -6.511002   1.888006 -19.096101  ...  10.651408  11.980405  -2.048504\n",
              "2  -3.281161  -1.488949   9.339293  ...   3.601074  -1.243163  -2.445833\n",
              "3  -4.083976   3.832610  11.235773  ...  11.188882   0.379290  12.553828\n",
              "4   3.921686  -9.364057  10.836048  ...   0.165564  -2.391553  16.111234\n",
              "5   3.699987 -20.871125  -0.121494  ...  -5.670873  13.170855  -9.200022\n",
              "6   2.166755   4.721834  12.354983  ... -10.216622   2.304482  -7.545882\n",
              "7  62.366266  14.330558  -6.262381  ...   2.437832   6.260088   4.897240\n",
              "8  -5.523902  38.602066   5.258477  ...  -7.714512   7.107791  -6.926707\n",
              "9   0.835655  10.300563  19.887098  ...   4.109268  -4.517253  -6.286026\n",
              "\n",
              "[10 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#feature extraction PCA\n",
        "from sklearn.decomposition import PCA  \n",
        "from pandas.core.frame import DataFrame\n",
        "\n",
        "pca_sk = PCA()\n",
        "newMat_pm_c2 = pca_sk.fit_transform(x_data_pm_c2)  \n",
        "\n",
        "data1 = DataFrame(newMat_pm_c2)\n",
        "data1.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "gLEsIHNq07_L",
        "outputId": "59a7750f-bc53-402e-fb14-a4938d1ba86a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-5c760684-3d8f-4177-bdb3-2bbf9f5a2e6a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-35.189676</td>\n",
              "      <td>-12.550456</td>\n",
              "      <td>0.136819</td>\n",
              "      <td>8.419146</td>\n",
              "      <td>-4.493815</td>\n",
              "      <td>-2.130318</td>\n",
              "      <td>-6.988130</td>\n",
              "      <td>23.955868</td>\n",
              "      <td>-7.417188</td>\n",
              "      <td>6.900235e-14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-36.263095</td>\n",
              "      <td>-8.457660</td>\n",
              "      <td>-8.500223</td>\n",
              "      <td>0.741427</td>\n",
              "      <td>-1.386311</td>\n",
              "      <td>-21.548427</td>\n",
              "      <td>18.672219</td>\n",
              "      <td>-4.514375</td>\n",
              "      <td>-7.430341</td>\n",
              "      <td>6.900235e-14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-36.612429</td>\n",
              "      <td>-11.213587</td>\n",
              "      <td>-13.974586</td>\n",
              "      <td>-13.844898</td>\n",
              "      <td>-5.632191</td>\n",
              "      <td>1.486171</td>\n",
              "      <td>4.337490</td>\n",
              "      <td>5.354534</td>\n",
              "      <td>20.391623</td>\n",
              "      <td>6.900235e-14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-34.140867</td>\n",
              "      <td>-15.347789</td>\n",
              "      <td>43.067791</td>\n",
              "      <td>2.116929</td>\n",
              "      <td>7.449036</td>\n",
              "      <td>3.035105</td>\n",
              "      <td>5.090911</td>\n",
              "      <td>-1.614669</td>\n",
              "      <td>4.169785</td>\n",
              "      <td>6.900235e-14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-35.814838</td>\n",
              "      <td>-16.743412</td>\n",
              "      <td>2.687118</td>\n",
              "      <td>-25.299704</td>\n",
              "      <td>-11.153988</td>\n",
              "      <td>-2.557568</td>\n",
              "      <td>-15.685029</td>\n",
              "      <td>-9.873125</td>\n",
              "      <td>-6.979084</td>\n",
              "      <td>6.900235e-14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>30.558776</td>\n",
              "      <td>69.111961</td>\n",
              "      <td>4.446718</td>\n",
              "      <td>10.551713</td>\n",
              "      <td>-22.828167</td>\n",
              "      <td>7.898143</td>\n",
              "      <td>3.767745</td>\n",
              "      <td>-2.996065</td>\n",
              "      <td>-0.470444</td>\n",
              "      <td>6.900235e-14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-23.628299</td>\n",
              "      <td>-17.243764</td>\n",
              "      <td>-9.961184</td>\n",
              "      <td>30.953780</td>\n",
              "      <td>5.050615</td>\n",
              "      <td>-4.581027</td>\n",
              "      <td>-12.561217</td>\n",
              "      <td>-10.262196</td>\n",
              "      <td>5.312054</td>\n",
              "      <td>6.900235e-14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>29.024631</td>\n",
              "      <td>69.502767</td>\n",
              "      <td>-3.118840</td>\n",
              "      <td>-10.506331</td>\n",
              "      <td>22.963909</td>\n",
              "      <td>-6.693053</td>\n",
              "      <td>-5.489940</td>\n",
              "      <td>2.209919</td>\n",
              "      <td>0.366466</td>\n",
              "      <td>6.900235e-14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>-30.272834</td>\n",
              "      <td>-14.875982</td>\n",
              "      <td>-14.898445</td>\n",
              "      <td>-1.165197</td>\n",
              "      <td>10.552572</td>\n",
              "      <td>26.007528</td>\n",
              "      <td>7.782171</td>\n",
              "      <td>-3.163731</td>\n",
              "      <td>-7.929733</td>\n",
              "      <td>6.900235e-14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>172.338631</td>\n",
              "      <td>-42.182077</td>\n",
              "      <td>0.114832</td>\n",
              "      <td>-1.966864</td>\n",
              "      <td>-0.521660</td>\n",
              "      <td>-0.916555</td>\n",
              "      <td>1.073780</td>\n",
              "      <td>0.903841</td>\n",
              "      <td>-0.013140</td>\n",
              "      <td>6.900235e-14</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5c760684-3d8f-4177-bdb3-2bbf9f5a2e6a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5c760684-3d8f-4177-bdb3-2bbf9f5a2e6a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5c760684-3d8f-4177-bdb3-2bbf9f5a2e6a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "            0          1          2  ...          7          8             9\n",
              "0  -35.189676 -12.550456   0.136819  ...  23.955868  -7.417188  6.900235e-14\n",
              "1  -36.263095  -8.457660  -8.500223  ...  -4.514375  -7.430341  6.900235e-14\n",
              "2  -36.612429 -11.213587 -13.974586  ...   5.354534  20.391623  6.900235e-14\n",
              "3  -34.140867 -15.347789  43.067791  ...  -1.614669   4.169785  6.900235e-14\n",
              "4  -35.814838 -16.743412   2.687118  ...  -9.873125  -6.979084  6.900235e-14\n",
              "5   30.558776  69.111961   4.446718  ...  -2.996065  -0.470444  6.900235e-14\n",
              "6  -23.628299 -17.243764  -9.961184  ... -10.262196   5.312054  6.900235e-14\n",
              "7   29.024631  69.502767  -3.118840  ...   2.209919   0.366466  6.900235e-14\n",
              "8  -30.272834 -14.875982 -14.898445  ...  -3.163731  -7.929733  6.900235e-14\n",
              "9  172.338631 -42.182077   0.114832  ...   0.903841  -0.013140  6.900235e-14\n",
              "\n",
              "[10 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# kmeans"
      ],
      "metadata": {
        "id": "2uJcVqIqBfKt"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "2so2M4HxGR47",
        "outputId": "7d67f730-27d7-4296-9ea1-daaab54b176f"
      },
      "source": [
        "# function returns WSS score for k values from 1 to kmax\n",
        "sse = []\n",
        "kmax = 25\n",
        "for i in range(1,kmax+1): \n",
        "    kmeans = KMeans(n_clusters=i, init ='k-means++', max_iter=2000, n_init=10,random_state=40 )\n",
        "    kmeans.fit(x_data)\n",
        "    centroids = kmeans.cluster_centers_\n",
        "    pred_clusters = kmeans.predict(x_data)\n",
        "    curr_sse = 0\n",
        "    \n",
        "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
        "    for i in range(len(x_data)):\n",
        "        curr_center = np.array(centroids[pred_clusters[i]])\n",
        "        curr_point = x_data[i]\n",
        "        curr_sse += sum((curr_point-curr_center)**2)\n",
        "        \n",
        "    sse.append(curr_sse)\n",
        "\n",
        "plt.figure(figsize=(10,7))     \n",
        "plt.xticks(fontsize= 20)\n",
        "plt.yticks(fontsize= 20)\n",
        "plt.plot(range(1,kmax+1),sse)\n",
        "plt.title('The Elbow Method Graph',fontsize= 30)\n",
        "plt.xlabel('Number of clusters',fontsize= 26)\n",
        "plt.ylabel('Within Cluster Sum of Squared Errors',fontsize= 26)\n",
        "#plt.savefig(\"elbow.pdf\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Dn2-5NW5NzB",
        "outputId": "a45481eb-50d9-447c-93c8-173a2f6629d8"
      },
      "source": [
        "#t-sne\n",
        "from sklearn.manifold import TSNE\n",
        "# 嵌入空间的维度为2，即将数据降维成2维\n",
        "ts = TSNE(n_components=3)\n",
        "# 训练模型\n",
        "ts.fit_transform(newMat)\n",
        "# 打印结果\n",
        "#print(ts.embedding_)\n",
        "ttest=np.asarray(ts.embedding_)\n",
        "print(ttest.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(128, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#t-sne\n",
        "from sklearn.manifold import TSNE\n",
        "# 嵌入空间的维度为2，即将数据降维成2维\n",
        "ts = TSNE(n_components=3)\n",
        "# 训练模型\n",
        "X_tsne=ts.fit_transform(newMat_pm)\n",
        "# 打印结果\n",
        "#print(ts.embedding_)\n",
        "ttest=np.asarray(ts.embedding_)\n",
        "print(ttest.shape)\n",
        "\n",
        "\n",
        "x_min, x_max = X_tsne.min(0), X_tsne.max(0)\n",
        "X_norm = (X_tsne - x_min) / (x_max - x_min)  # 归一化\n",
        "plt.figure(figsize=(8, 8))\n",
        "for i in range(X_norm.shape[0]):\n",
        "    plt.text(X_norm[i, 0], X_norm[i, 1], X_norm[i, 2],\n",
        "             fontdict={'weight': 'bold', 'size': 9})\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "3Tgk4qpBxakw",
        "outputId": "46741624-e621-40e9-e661-22676be8e2cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(31, 3)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HwUBDFut5hSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNHwwH0AjCkz"
      },
      "source": [
        "#K_mean\n",
        "from sklearn.cluster import KMeans\n",
        "cluster=KMeans(n_clusters=20).fit(ttest)\n",
        "\n",
        "kmeans = KMeans(n_clusters=18, init ='k-means++', max_iter=2000,  n_init=10, random_state=40 )\n",
        "kmeans.fit_predict(ttest)\n",
        "# save the clustered labels\n",
        "data_cluster = pd.DataFrame(kmeans.labels_,columns=[\"clusterid\"])\n",
        "data_cluster['tsne-2d-one'] = ttest[:,0]\n",
        "data_cluster['tsne-2d-two'] = ttest[:,1]\n",
        "data_cluster['tsne-2d-three'] = ttest[:,2]\n",
        "\n",
        "data_cluster\n",
        "ttest.shape\n",
        "\n",
        "#画图\n",
        "sns.set_style(\"white\")\n",
        "plt.figure(figsize=(10,7))\n",
        "\n",
        "sns.set(font_scale=1.1)\n",
        "sns.scatterplot(\n",
        "    x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n",
        "    hue=\"clusterid\",\n",
        "    palette=sns.color_palette(\"hls\", 18),\n",
        "    data=data_cluster,\n",
        "    legend=\"full\",\n",
        "    alpha=0.5,\n",
        ")\n",
        "\n",
        "centroid=cluster.cluster_centers_\n",
        "centroid # 查看质心\n",
        "\n",
        "centroid.shape\n",
        "newC=centroid.T\n",
        "newC.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# copy data(not used)"
      ],
      "metadata": {
        "id": "IaYwJ5pkBlkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "remain_data=newMat_sev[0:30]\n",
        "newMat_sev_m=newMat_sev[30:]\n",
        "newMat_sev_m.shape\n",
        "large_x=np.vstack((newMat_sev_m,newMat_sev_m))\n",
        "large_x=np.vstack((large_x,large_x))\n",
        "print(large_x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LSdDZm8OL6v",
        "outputId": "3ce9fce2-0317-43b4-807e-1e8efd082320"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(288, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "remain_data_y=y_sev_label[0:30]\n",
        "y_sev_label_m=y_sev_label[30:]\n",
        "y_sev_label_m.shape\n",
        "large_y=np.concatenate((y_sev_label_m,y_sev_label_m))\n",
        "large_y=np.concatenate((large_y,large_y))\n",
        "large_y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsrK0UmRRavC",
        "outputId": "2fa3db95-1fa3-444d-e75b-823b165bf3bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(288,)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP for two task"
      ],
      "metadata": {
        "id": "AGzHRkB8B6i2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        " \n",
        "auc=[]\n",
        "accu=[]\n",
        "f1=[]\n",
        "\n",
        "X=newMat_RNA#x_datalarge_x#x_covid##newC#newMat#x_data\n",
        "y=y_label_RNA\n",
        "input_shape=X.shape[1]\n",
        "\n",
        "sf=StratifiedKFold(n_splits=3, random_state=1, shuffle=True)\n",
        "\n",
        "for train_ix , test_ix in sf.split(X, y):\n",
        "\n",
        "  \n",
        "  model = Sequential()\n",
        "  model.add(Dense(128, input_dim=input_shape, activation='sigmoid'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(128, activation='sigmoid'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(64, activation='sigmoid'))\n",
        "  model.add(Dense(3,activation='softmax'))\n",
        " \n",
        "  model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
        "\n",
        "  train_x,test_x = X[train_ix],X[test_ix]\n",
        "  train_y,test_y = np.array(y)[train_ix],np.array(y)[test_ix]\n",
        "  #train_y= np_utils.to_categorical(train_y,3)\n",
        "  #test_y = np_utils.to_categorical(test_y,3)\n",
        "\n",
        "  oversample=SMOTE()\n",
        "  train_x,train_y=oversample.fit_resample(train_x, train_y)\n",
        "  model.fit(train_x,train_y,epochs=40,batch_size=10,verbose=0)\n",
        "\n",
        "  y_pred=model.predict(test_x,batch_size=10)\n",
        "  y_pred-np.asarray(y_pred)\n",
        "\n",
        "  score = model.evaluate(test_x,test_y, batch_size=20)\n",
        "  auc.append(score[1])\n",
        "  f1=f1_score(test_y,y_pred.round(),average='micro')\n",
        "  accu.append(f1)\n",
        "\n",
        "auc=mean(auc)\n",
        "accu=np.asarray(accu)\n",
        "accu=mean(accu)\n",
        "\n",
        "#remain_data_y_cate=np_utils.to_categorical(remain_data_y,3)\n",
        "score_test=model.evaluate(remain_data,remain_data_y_cate,batch_size=10)\n",
        "\n",
        "print(\"test_score:loss&accuracy\",score_test)\n",
        "print('accuracy',auc)\n",
        "print('f1',accu)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 974
        },
        "id": "CMBj0I72XvmP",
        "outputId": "a7384239-d90d-4b6c-e2ad-13f1f7b18edd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-d5e10d65cacd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0moversample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSMOTE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moversample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 919, in compute_loss\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 1790, in categorical_crossentropy\n        y_true, y_pred, from_logits=from_logits, axis=axis)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/backend.py\", line 5083, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (10, 1) and (10, 3) are incompatible\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#MLP for RNA\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "auc=[]\n",
        "accu=[]\n",
        "f1score=[]\n",
        "C_sum=np.zeros((2,2))\n",
        "\n",
        "X=x_data_RNA#newMat_RNA##x_datalarge_x#x_covid##newC#newMat#x_data#newMat_RNA\n",
        "y=y_label_RNA\n",
        "input_shape=X.shape[1]\n",
        "\n",
        "n_splits_num=5\n",
        "sf=StratifiedKFold(n_splits=n_splits_num,random_state=1,shuffle=True)\n",
        "\n",
        "for train_ix , test_ix in sf.split(X, y):\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Dense(256, input_dim=input_shape, activation='sigmoid'))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(128, activation='sigmoid'))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(64, activation='sigmoid'))\n",
        "  model.add(Dense(32, activation='sigmoid'))\n",
        "  model.add(Dense(1,activation='sigmoid'))\n",
        " \n",
        "  model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
        "\n",
        "  train_x,test_x = X[train_ix],X[test_ix]\n",
        "  train_y,test_y = np.array(y)[train_ix],np.array(y)[test_ix]\n",
        "\n",
        "  oversample=SMOTE()\n",
        "  train_x,train_y=oversample.fit_resample(train_x, train_y)\n",
        "  model.fit(train_x,train_y,epochs=20,batch_size=5,verbose=1)\n",
        "\n",
        "\n",
        "  score = model.evaluate(test_x,test_y, batch_size=5)\n",
        "  y_pred=model.predict(test_x,batch_size=5)\n",
        "  y_pred-np.asarray(y_pred)\n",
        "  \n",
        "  accuracy=accuracy_score(test_y,y_pred.round())\n",
        "  f1=f1_score(test_y,y_pred.round())\n",
        "  C=confusion_matrix(test_y,y_pred.round())\n",
        "\n",
        "  C_sum+=C\n",
        "  print('C',C)\n",
        "\n",
        "  f1score.append(f1)\n",
        "  accu.append(f1)\n",
        "  auc.append(score[1])\n",
        "  \n",
        "\n",
        "auc=mean(auc)\n",
        "accu=np.asarray(accu)\n",
        "accu=mean(accu)\n",
        "f1score=mean(f1score)\n",
        "\n",
        "print('auc',auc)\n",
        "print('f1',f1score)\n",
        "print('accu',accu)\n",
        "\n",
        "print(C_sum)\n",
        "print(C_sum/n_splits_num)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCm3afJzY-es",
        "outputId": "451c927d-f81a-420f-9d25-946583e086d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "14/14 [==============================] - 1s 44ms/step - loss: 0.7867 - accuracy: 0.5000\n",
            "Epoch 2/20\n",
            "14/14 [==============================] - 1s 42ms/step - loss: 0.7088 - accuracy: 0.4286\n",
            "Epoch 3/20\n",
            "14/14 [==============================] - 1s 43ms/step - loss: 0.6952 - accuracy: 0.5286\n",
            "Epoch 4/20\n",
            "14/14 [==============================] - 1s 52ms/step - loss: 0.7031 - accuracy: 0.4000\n",
            "Epoch 5/20\n",
            "14/14 [==============================] - 1s 52ms/step - loss: 0.7053 - accuracy: 0.4429\n",
            "Epoch 6/20\n",
            "14/14 [==============================] - 1s 45ms/step - loss: 0.7052 - accuracy: 0.3429\n",
            "Epoch 7/20\n",
            "14/14 [==============================] - 1s 54ms/step - loss: 0.7015 - accuracy: 0.3857\n",
            "Epoch 8/20\n",
            "14/14 [==============================] - 1s 67ms/step - loss: 0.7021 - accuracy: 0.4429\n",
            "Epoch 9/20\n",
            "14/14 [==============================] - 1s 61ms/step - loss: 0.6987 - accuracy: 0.5429\n",
            "Epoch 10/20\n",
            "14/14 [==============================] - 1s 41ms/step - loss: 0.7015 - accuracy: 0.4286\n",
            "Epoch 11/20\n",
            "14/14 [==============================] - 1s 41ms/step - loss: 0.7047 - accuracy: 0.4857\n",
            "Epoch 12/20\n",
            "14/14 [==============================] - 1s 43ms/step - loss: 0.6974 - accuracy: 0.4429\n",
            "Epoch 13/20\n",
            "14/14 [==============================] - 1s 41ms/step - loss: 0.7028 - accuracy: 0.4000\n",
            "Epoch 14/20\n",
            "14/14 [==============================] - 1s 42ms/step - loss: 0.7011 - accuracy: 0.4286\n",
            "Epoch 15/20\n",
            "14/14 [==============================] - 1s 41ms/step - loss: 0.6991 - accuracy: 0.4571\n",
            "Epoch 16/20\n",
            "14/14 [==============================] - 1s 43ms/step - loss: 0.7009 - accuracy: 0.4000\n",
            "Epoch 17/20\n",
            "14/14 [==============================] - 1s 44ms/step - loss: 0.7031 - accuracy: 0.4000\n",
            "Epoch 18/20\n",
            "14/14 [==============================] - 1s 41ms/step - loss: 0.7059 - accuracy: 0.4000\n",
            "Epoch 19/20\n",
            "14/14 [==============================] - 1s 42ms/step - loss: 0.7019 - accuracy: 0.3714\n",
            "Epoch 20/20\n",
            "14/14 [==============================] - 1s 42ms/step - loss: 0.6953 - accuracy: 0.5143\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.7173 - accuracy: 0.1818\n",
            "C [[2 0]\n",
            " [9 0]]\n",
            "Epoch 1/20\n",
            "14/14 [==============================] - 1s 43ms/step - loss: 0.7237 - accuracy: 0.5000\n",
            "Epoch 2/20\n",
            "14/14 [==============================] - 1s 43ms/step - loss: 0.6959 - accuracy: 0.5429\n",
            "Epoch 3/20\n",
            "14/14 [==============================] - 1s 44ms/step - loss: 0.7060 - accuracy: 0.4286\n",
            "Epoch 4/20\n",
            "14/14 [==============================] - 1s 44ms/step - loss: 0.7074 - accuracy: 0.4286\n",
            "Epoch 5/20\n",
            "14/14 [==============================] - 1s 43ms/step - loss: 0.6986 - accuracy: 0.4857\n",
            "Epoch 6/20\n",
            "14/14 [==============================] - 1s 42ms/step - loss: 0.7012 - accuracy: 0.4857\n",
            "Epoch 7/20\n",
            "14/14 [==============================] - 1s 41ms/step - loss: 0.7050 - accuracy: 0.4429\n",
            "Epoch 8/20\n",
            "14/14 [==============================] - 1s 39ms/step - loss: 0.7134 - accuracy: 0.3429\n",
            "Epoch 9/20\n",
            "14/14 [==============================] - 1s 37ms/step - loss: 0.7025 - accuracy: 0.4857\n",
            "Epoch 10/20\n",
            "14/14 [==============================] - 1s 38ms/step - loss: 0.7018 - accuracy: 0.4286\n",
            "Epoch 11/20\n",
            "14/14 [==============================] - 1s 37ms/step - loss: 0.7037 - accuracy: 0.4429\n",
            "Epoch 12/20\n",
            "14/14 [==============================] - 1s 37ms/step - loss: 0.7034 - accuracy: 0.4857\n",
            "Epoch 13/20\n",
            "14/14 [==============================] - 1s 37ms/step - loss: 0.7057 - accuracy: 0.4286\n",
            "Epoch 14/20\n",
            "14/14 [==============================] - 1s 38ms/step - loss: 0.6985 - accuracy: 0.5000\n",
            "Epoch 15/20\n",
            "14/14 [==============================] - 1s 37ms/step - loss: 0.7012 - accuracy: 0.4143\n",
            "Epoch 16/20\n",
            "14/14 [==============================] - 1s 37ms/step - loss: 0.6980 - accuracy: 0.4429\n",
            "Epoch 17/20\n",
            "14/14 [==============================] - 1s 37ms/step - loss: 0.6960 - accuracy: 0.5000\n",
            "Epoch 18/20\n",
            "14/14 [==============================] - 1s 38ms/step - loss: 0.7071 - accuracy: 0.4429\n",
            "Epoch 19/20\n",
            "14/14 [==============================] - 1s 37ms/step - loss: 0.6970 - accuracy: 0.4714\n",
            "Epoch 20/20\n",
            "14/14 [==============================] - 1s 38ms/step - loss: 0.6963 - accuracy: 0.5000\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.6773 - accuracy: 0.8182\n",
            "C [[0 2]\n",
            " [0 9]]\n",
            "Epoch 1/20\n",
            "14/14 [==============================] - 1s 41ms/step - loss: 0.7182 - accuracy: 0.4571\n",
            "Epoch 2/20\n",
            "14/14 [==============================] - 1s 40ms/step - loss: 0.7050 - accuracy: 0.4143\n",
            "Epoch 3/20\n",
            "14/14 [==============================] - 1s 42ms/step - loss: 0.6993 - accuracy: 0.5000\n",
            "Epoch 4/20\n",
            "14/14 [==============================] - 1s 41ms/step - loss: 0.7034 - accuracy: 0.4286\n",
            "Epoch 5/20\n",
            "14/14 [==============================] - 1s 37ms/step - loss: 0.6963 - accuracy: 0.5143\n",
            "Epoch 6/20\n",
            "14/14 [==============================] - 1s 41ms/step - loss: 0.7069 - accuracy: 0.4286\n",
            "Epoch 7/20\n",
            "14/14 [==============================] - 1s 42ms/step - loss: 0.6982 - accuracy: 0.4571\n",
            "Epoch 8/20\n",
            "14/14 [==============================] - 1s 42ms/step - loss: 0.7000 - accuracy: 0.5286\n",
            "Epoch 9/20\n",
            "14/14 [==============================] - 1s 44ms/step - loss: 0.7003 - accuracy: 0.4714\n",
            "Epoch 10/20\n",
            "14/14 [==============================] - 1s 42ms/step - loss: 0.7099 - accuracy: 0.4143\n",
            "Epoch 11/20\n",
            "14/14 [==============================] - 1s 39ms/step - loss: 0.7029 - accuracy: 0.5000\n",
            "Epoch 12/20\n",
            "14/14 [==============================] - 1s 39ms/step - loss: 0.7003 - accuracy: 0.4429\n",
            "Epoch 13/20\n",
            "14/14 [==============================] - 1s 42ms/step - loss: 0.6995 - accuracy: 0.4857\n",
            "Epoch 14/20\n",
            "14/14 [==============================] - 1s 42ms/step - loss: 0.6965 - accuracy: 0.5286\n",
            "Epoch 15/20\n",
            "14/14 [==============================] - 1s 45ms/step - loss: 0.7018 - accuracy: 0.4000\n",
            "Epoch 16/20\n",
            "14/14 [==============================] - 1s 45ms/step - loss: 0.7040 - accuracy: 0.4286\n",
            "Epoch 17/20\n",
            "14/14 [==============================] - 1s 43ms/step - loss: 0.7033 - accuracy: 0.3714\n",
            "Epoch 18/20\n",
            "14/14 [==============================] - 1s 40ms/step - loss: 0.7038 - accuracy: 0.4714\n",
            "Epoch 19/20\n",
            "14/14 [==============================] - 1s 43ms/step - loss: 0.6994 - accuracy: 0.4429\n",
            "Epoch 20/20\n",
            "14/14 [==============================] - 1s 43ms/step - loss: 0.7024 - accuracy: 0.4429\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.7016 - accuracy: 0.1818\n",
            "C [[2 0]\n",
            " [9 0]]\n",
            "Epoch 1/20\n",
            "14/14 [==============================] - 1s 40ms/step - loss: 0.7041 - accuracy: 0.5571\n",
            "Epoch 2/20\n",
            "14/14 [==============================] - 1s 42ms/step - loss: 0.7006 - accuracy: 0.5000\n",
            "Epoch 3/20\n",
            "14/14 [==============================] - 1s 45ms/step - loss: 0.7006 - accuracy: 0.4571\n",
            "Epoch 4/20\n",
            "14/14 [==============================] - 1s 44ms/step - loss: 0.7041 - accuracy: 0.3571\n",
            "Epoch 5/20\n",
            "14/14 [==============================] - 1s 44ms/step - loss: 0.7025 - accuracy: 0.4143\n",
            "Epoch 6/20\n",
            "14/14 [==============================] - 1s 40ms/step - loss: 0.7003 - accuracy: 0.4714\n",
            "Epoch 7/20\n",
            "14/14 [==============================] - 1s 40ms/step - loss: 0.7024 - accuracy: 0.4286\n",
            "Epoch 8/20\n",
            "14/14 [==============================] - 1s 38ms/step - loss: 0.6982 - accuracy: 0.4286\n",
            "Epoch 9/20\n",
            "14/14 [==============================] - 1s 38ms/step - loss: 0.7014 - accuracy: 0.5143\n",
            "Epoch 10/20\n",
            "14/14 [==============================] - 1s 41ms/step - loss: 0.7033 - accuracy: 0.4286\n",
            "Epoch 11/20\n",
            "14/14 [==============================] - 1s 39ms/step - loss: 0.7028 - accuracy: 0.3429\n",
            "Epoch 12/20\n",
            "14/14 [==============================] - 1s 39ms/step - loss: 0.7015 - accuracy: 0.3714\n",
            "Epoch 13/20\n",
            "14/14 [==============================] - 1s 40ms/step - loss: 0.7025 - accuracy: 0.3857\n",
            "Epoch 14/20\n",
            "14/14 [==============================] - 1s 38ms/step - loss: 0.6996 - accuracy: 0.4286\n",
            "Epoch 15/20\n",
            "14/14 [==============================] - 1s 39ms/step - loss: 0.6988 - accuracy: 0.5571\n",
            "Epoch 16/20\n",
            "14/14 [==============================] - 1s 38ms/step - loss: 0.7010 - accuracy: 0.4000\n",
            "Epoch 17/20\n",
            "14/14 [==============================] - 1s 38ms/step - loss: 0.7016 - accuracy: 0.4857\n",
            "Epoch 18/20\n",
            "14/14 [==============================] - 1s 38ms/step - loss: 0.7049 - accuracy: 0.3714\n",
            "Epoch 19/20\n",
            "14/14 [==============================] - 1s 38ms/step - loss: 0.6952 - accuracy: 0.4429\n",
            "Epoch 20/20\n",
            "14/14 [==============================] - 1s 38ms/step - loss: 0.7012 - accuracy: 0.4286\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.6763 - accuracy: 0.8182\n",
            "C [[0 2]\n",
            " [0 9]]\n",
            "Epoch 1/20\n",
            "15/15 [==============================] - 1s 40ms/step - loss: 0.7044 - accuracy: 0.5278\n",
            "Epoch 2/20\n",
            "15/15 [==============================] - 1s 40ms/step - loss: 0.7095 - accuracy: 0.4861\n",
            "Epoch 3/20\n",
            "15/15 [==============================] - 1s 40ms/step - loss: 0.6958 - accuracy: 0.5278\n",
            "Epoch 4/20\n",
            "15/15 [==============================] - 1s 40ms/step - loss: 0.7051 - accuracy: 0.4722\n",
            "Epoch 5/20\n",
            "15/15 [==============================] - 1s 41ms/step - loss: 0.7107 - accuracy: 0.4167\n",
            "Epoch 6/20\n",
            "15/15 [==============================] - 1s 40ms/step - loss: 0.7029 - accuracy: 0.4306\n",
            "Epoch 7/20\n",
            "15/15 [==============================] - 1s 40ms/step - loss: 0.6991 - accuracy: 0.5000\n",
            "Epoch 8/20\n",
            "15/15 [==============================] - 1s 41ms/step - loss: 0.7099 - accuracy: 0.3889\n",
            "Epoch 9/20\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 0.7017 - accuracy: 0.4444\n",
            "Epoch 10/20\n",
            "15/15 [==============================] - 1s 45ms/step - loss: 0.6994 - accuracy: 0.4861\n",
            "Epoch 11/20\n",
            "15/15 [==============================] - 1s 47ms/step - loss: 0.7064 - accuracy: 0.5000\n",
            "Epoch 12/20\n",
            "15/15 [==============================] - 1s 47ms/step - loss: 0.7024 - accuracy: 0.4583\n",
            "Epoch 13/20\n",
            "15/15 [==============================] - 1s 46ms/step - loss: 0.7042 - accuracy: 0.4861\n",
            "Epoch 14/20\n",
            "15/15 [==============================] - 1s 44ms/step - loss: 0.6944 - accuracy: 0.5417\n",
            "Epoch 15/20\n",
            "15/15 [==============================] - 1s 49ms/step - loss: 0.7031 - accuracy: 0.5000\n",
            "Epoch 16/20\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 0.7023 - accuracy: 0.4306\n",
            "Epoch 17/20\n",
            "15/15 [==============================] - 1s 44ms/step - loss: 0.7012 - accuracy: 0.4028\n",
            "Epoch 18/20\n",
            "15/15 [==============================] - 1s 41ms/step - loss: 0.7002 - accuracy: 0.5000\n",
            "Epoch 19/20\n",
            "15/15 [==============================] - 1s 41ms/step - loss: 0.6976 - accuracy: 0.4306\n",
            "Epoch 20/20\n",
            "15/15 [==============================] - 1s 41ms/step - loss: 0.6975 - accuracy: 0.4722\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.7266 - accuracy: 0.2000\n",
            "C [[2 0]\n",
            " [8 0]]\n",
            "auc 0.44000000059604644\n",
            "f1 0.36\n",
            "accu 0.36\n",
            "[[ 6.  4.]\n",
            " [26. 18.]]\n",
            "[[1.2 0.8]\n",
            " [5.2 3.6]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#mlp for pm\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "auc=[]\n",
        "accu=[]\n",
        "f1=[]\n",
        "\n",
        "X=newMat_pm#x_data_pm#x_datalarge_x#x_covid##newC#newMat#x_data\n",
        "y=y_label_pm\n",
        "input_shape=X.shape[1]\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=input_shape, activation='sigmoid'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(64, activation='sigmoid'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(32, activation='sigmoid'))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
        "\n",
        "oversample=SMOTE()\n",
        "train_x,train_y=oversample.fit_resample(X,y)\n",
        "model.fit(train_x,train_y,epochs=30,batch_size=5,verbose=1)\n",
        "\n",
        "score = model.evaluate(train_x,train_y, batch_size=5)\n",
        "print(score)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwAR3uoURN7y",
        "outputId": "270ed39b-a202-4bd2-c840-33a62245cdfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "8/8 [==============================] - 1s 2ms/step - loss: 0.7328 - accuracy: 0.4444\n",
            "Epoch 2/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.7179 - accuracy: 0.4444\n",
            "Epoch 3/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.7014 - accuracy: 0.5000\n",
            "Epoch 4/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6840 - accuracy: 0.6111\n",
            "Epoch 5/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6812 - accuracy: 0.5833\n",
            "Epoch 6/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6851 - accuracy: 0.5000\n",
            "Epoch 7/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6659 - accuracy: 0.6667\n",
            "Epoch 8/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6545 - accuracy: 0.7500\n",
            "Epoch 9/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6446 - accuracy: 0.6944\n",
            "Epoch 10/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6370 - accuracy: 0.6389\n",
            "Epoch 11/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6186 - accuracy: 0.6389\n",
            "Epoch 12/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6213 - accuracy: 0.8056\n",
            "Epoch 13/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6097 - accuracy: 0.6667\n",
            "Epoch 14/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5791 - accuracy: 0.8333\n",
            "Epoch 15/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5731 - accuracy: 0.8056\n",
            "Epoch 16/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5624 - accuracy: 0.8056\n",
            "Epoch 17/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5337 - accuracy: 0.8333\n",
            "Epoch 18/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5303 - accuracy: 0.8333\n",
            "Epoch 19/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4985 - accuracy: 0.8611\n",
            "Epoch 20/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4929 - accuracy: 0.8056\n",
            "Epoch 21/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4769 - accuracy: 0.8333\n",
            "Epoch 22/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4512 - accuracy: 0.8889\n",
            "Epoch 23/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4498 - accuracy: 0.8333\n",
            "Epoch 24/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4052 - accuracy: 0.8889\n",
            "Epoch 25/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4258 - accuracy: 0.8611\n",
            "Epoch 26/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3997 - accuracy: 0.8889\n",
            "Epoch 27/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3527 - accuracy: 0.9167\n",
            "Epoch 28/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3721 - accuracy: 0.8611\n",
            "Epoch 29/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3479 - accuracy: 0.9167\n",
            "Epoch 30/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3253 - accuracy: 0.8889\n",
            "WARNING:tensorflow:5 out of the last 14 calls to <function Model.make_test_function.<locals>.test_function at 0x7f54fdaab7a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3096 - accuracy: 0.8611\n",
            "[0.30955958366394043, 0.8611111044883728]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=model.predict(newMat_pm_c2)\n",
        "y_pred-np.asarray(y_pred)\n",
        "\n",
        "accuracy=accuracy_score(y_label_pm_c2,y_pred.round())\n",
        "f1=f1_score(y_label_pm_c2,y_pred.round())\n",
        "print(f1)\n",
        "print(accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0FSrNYx1GcS",
        "outputId": "940f39bf-b6e7-4d16-fa46-1dbf1d3eeeb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4444444444444445\n",
            "0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#MLP for RNA\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "auc=[]\n",
        "accu=[]\n",
        "f1=[]\n",
        "\n",
        "X=newMat_RNA#x_datalarge_x#x_covid##newC#newMat#x_data\n",
        "y=y_label_RNA\n",
        "input_shape=X.shape[1]\n",
        "\n",
        "sf=StratifiedKFold(n_splits=10,random_state=1,shuffle=True)\n",
        "\n",
        "for train_ix , test_ix in sf.split(X, y):\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Dense(64, input_dim=input_shape, activation='sigmoid'))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(64, activation='sigmoid'))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(32, activation='sigmoid'))\n",
        "  model.add(Dense(1,activation='sigmoid'))\n",
        " \n",
        "  model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
        "\n",
        "  train_x,test_x = X[train_ix],X[test_ix]\n",
        "  train_y,test_y = np.array(y)[train_ix],np.array(y)[test_ix]\n",
        "\n",
        "  oversample=SMOTE()\n",
        "  train_x,train_y=oversample.fit_resample(train_x, train_y)\n",
        "  model.fit(train_x,train_y,epochs=20,batch_size=5,verbose=1)\n",
        "\n",
        "\n",
        "  score = model.evaluate(test_x,test_y, batch_size=5)\n",
        "  y_pred=model.predict(test_x,batch_size=5)\n",
        "  y_pred-np.asarray(y_pred)\n",
        "  \n",
        "  accuracy=accuracy_score(test_y,y_pred.round())\n",
        "  f1=f1_score(test_y,y_pred.round())\n",
        "\n",
        "  accu.append(accuracy)\n",
        "  f1.append(f1)\n",
        "  auc.append(score[1])\n",
        "  \n",
        "\n",
        "auc=mean(auc)\n",
        "accu=np.asarray(accu)\n",
        "accu=mean(accu)\n",
        "\n",
        "print('accuracy',auc)\n",
        "print('f1',accu)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41927280-104d-4b48-86c1-b7d432037690",
        "id": "TWfEsYiSQRSg"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "16/16 [==============================] - 1s 2ms/step - loss: 0.7249 - accuracy: 0.5000\n",
            "Epoch 2/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6315 - accuracy: 0.7308\n",
            "Epoch 3/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.5801 - accuracy: 0.9487\n",
            "Epoch 4/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.5008 - accuracy: 0.9872\n",
            "Epoch 5/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.4197 - accuracy: 0.9872\n",
            "Epoch 6/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.3388 - accuracy: 1.0000\n",
            "Epoch 7/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2600 - accuracy: 1.0000\n",
            "Epoch 8/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.1946 - accuracy: 1.0000\n",
            "Epoch 9/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.1516 - accuracy: 1.0000\n",
            "Epoch 10/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.1033 - accuracy: 1.0000\n",
            "Epoch 11/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0722 - accuracy: 1.0000\n",
            "Epoch 12/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0492 - accuracy: 1.0000\n",
            "Epoch 13/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0364 - accuracy: 1.0000\n",
            "Epoch 14/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0245 - accuracy: 1.0000\n",
            "Epoch 15/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0206 - accuracy: 1.0000\n",
            "Epoch 16/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0136 - accuracy: 1.0000\n",
            "Epoch 17/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0107 - accuracy: 1.0000\n",
            "Epoch 18/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0085 - accuracy: 1.0000\n",
            "Epoch 19/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0065 - accuracy: 1.0000\n",
            "Epoch 20/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0048 - accuracy: 1.0000\n",
            "WARNING:tensorflow:6 out of the last 10 calls to <function Model.make_test_function.<locals>.test_function at 0x7f8608cc1050> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.0077 - accuracy: 1.0000\n",
            "WARNING:tensorflow:6 out of the last 10 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8608a73e60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "acc 1.0\n",
            "Epoch 1/20\n",
            "16/16 [==============================] - 1s 2ms/step - loss: 0.6911 - accuracy: 0.5513\n",
            "Epoch 2/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6396 - accuracy: 0.7692\n",
            "Epoch 3/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.5957 - accuracy: 0.9103\n",
            "Epoch 4/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.5197 - accuracy: 0.9744\n",
            "Epoch 5/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.4516 - accuracy: 0.9744\n",
            "Epoch 6/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.4003 - accuracy: 0.9872\n",
            "Epoch 7/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.3364 - accuracy: 0.9487\n",
            "Epoch 8/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2612 - accuracy: 0.9872\n",
            "Epoch 9/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.1991 - accuracy: 0.9872\n",
            "Epoch 10/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.1521 - accuracy: 0.9872\n",
            "Epoch 11/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.1224 - accuracy: 1.0000\n",
            "Epoch 12/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0898 - accuracy: 1.0000\n",
            "Epoch 13/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0690 - accuracy: 0.9872\n",
            "Epoch 14/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0466 - accuracy: 1.0000\n",
            "Epoch 15/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0396 - accuracy: 1.0000\n",
            "Epoch 16/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0310 - accuracy: 1.0000\n",
            "Epoch 17/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0315 - accuracy: 0.9872\n",
            "Epoch 18/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0175 - accuracy: 1.0000\n",
            "Epoch 19/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0161 - accuracy: 1.0000\n",
            "Epoch 20/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0403 - accuracy: 0.9872\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.0190 - accuracy: 1.0000\n",
            "acc 1.0\n",
            "Epoch 1/20\n",
            "16/16 [==============================] - 1s 2ms/step - loss: 0.6865 - accuracy: 0.5769\n",
            "Epoch 2/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6101 - accuracy: 0.7821\n",
            "Epoch 3/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.5233 - accuracy: 0.9872\n",
            "Epoch 4/20\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.4595 - accuracy: 0.9744\n",
            "Epoch 5/20\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.3775 - accuracy: 1.0000\n",
            "Epoch 6/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2966 - accuracy: 1.0000\n",
            "Epoch 7/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2192 - accuracy: 1.0000\n",
            "Epoch 8/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.1664 - accuracy: 0.9872\n",
            "Epoch 9/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.1212 - accuracy: 1.0000\n",
            "Epoch 10/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0909 - accuracy: 1.0000\n",
            "Epoch 11/20\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.0669 - accuracy: 1.0000\n",
            "Epoch 12/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0463 - accuracy: 1.0000\n",
            "Epoch 13/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0379 - accuracy: 1.0000\n",
            "Epoch 14/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0252 - accuracy: 1.0000\n",
            "Epoch 15/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0203 - accuracy: 1.0000\n",
            "Epoch 16/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0157 - accuracy: 1.0000\n",
            "Epoch 17/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0123 - accuracy: 1.0000\n",
            "Epoch 18/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0088 - accuracy: 1.0000\n",
            "Epoch 19/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0070 - accuracy: 1.0000\n",
            "Epoch 20/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0052 - accuracy: 1.0000\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.0067 - accuracy: 1.0000\n",
            "acc 1.0\n",
            "Epoch 1/20\n",
            "16/16 [==============================] - 1s 2ms/step - loss: 0.6918 - accuracy: 0.5128\n",
            "Epoch 2/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6368 - accuracy: 0.7051\n",
            "Epoch 3/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.5842 - accuracy: 0.9231\n",
            "Epoch 4/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.5458 - accuracy: 0.9103\n",
            "Epoch 5/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.4692 - accuracy: 0.9744\n",
            "Epoch 6/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.3960 - accuracy: 1.0000\n",
            "Epoch 7/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.3281 - accuracy: 1.0000\n",
            "Epoch 8/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2553 - accuracy: 1.0000\n",
            "Epoch 9/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.1904 - accuracy: 1.0000\n",
            "Epoch 10/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.1334 - accuracy: 1.0000\n",
            "Epoch 11/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.1032 - accuracy: 1.0000\n",
            "Epoch 12/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0799 - accuracy: 1.0000\n",
            "Epoch 13/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0606 - accuracy: 1.0000\n",
            "Epoch 14/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0406 - accuracy: 1.0000\n",
            "Epoch 15/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0309 - accuracy: 1.0000\n",
            "Epoch 16/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0236 - accuracy: 1.0000\n",
            "Epoch 17/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0192 - accuracy: 1.0000\n",
            "Epoch 18/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0135 - accuracy: 1.0000\n",
            "Epoch 19/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0108 - accuracy: 1.0000\n",
            "Epoch 20/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0083 - accuracy: 1.0000\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 0.0105 - accuracy: 1.0000\n",
            "acc 1.0\n",
            "Epoch 1/20\n",
            "16/16 [==============================] - 1s 2ms/step - loss: 0.6922 - accuracy: 0.5000\n",
            "Epoch 2/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6251 - accuracy: 0.8500\n",
            "Epoch 3/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.5816 - accuracy: 0.8500\n",
            "Epoch 4/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.5097 - accuracy: 0.9375\n",
            "Epoch 5/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.4196 - accuracy: 0.9875\n",
            "Epoch 6/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.3348 - accuracy: 0.9750\n",
            "Epoch 7/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2639 - accuracy: 1.0000\n",
            "Epoch 8/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2072 - accuracy: 0.9875\n",
            "Epoch 9/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.1439 - accuracy: 1.0000\n",
            "Epoch 10/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.1067 - accuracy: 1.0000\n",
            "Epoch 11/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0768 - accuracy: 1.0000\n",
            "Epoch 12/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0568 - accuracy: 1.0000\n",
            "Epoch 13/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0402 - accuracy: 1.0000\n",
            "Epoch 14/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0285 - accuracy: 1.0000\n",
            "Epoch 15/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0207 - accuracy: 1.0000\n",
            "Epoch 16/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0150 - accuracy: 1.0000\n",
            "Epoch 17/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0158 - accuracy: 1.0000\n",
            "Epoch 18/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0088 - accuracy: 1.0000\n",
            "Epoch 19/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0071 - accuracy: 1.0000\n",
            "Epoch 20/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0053 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 483ms/step - loss: 0.0252 - accuracy: 1.0000\n",
            "acc 1.0\n",
            "Epoch 1/20\n",
            "16/16 [==============================] - 1s 2ms/step - loss: 0.6792 - accuracy: 0.5875\n",
            "Epoch 2/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6272 - accuracy: 0.7375\n",
            "Epoch 3/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.5756 - accuracy: 0.9375\n",
            "Epoch 4/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.5011 - accuracy: 0.9750\n",
            "Epoch 5/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.4257 - accuracy: 0.9750\n",
            "Epoch 6/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.3290 - accuracy: 0.9875\n",
            "Epoch 7/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2642 - accuracy: 1.0000\n",
            "Epoch 8/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.1908 - accuracy: 1.0000\n",
            "Epoch 9/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.1483 - accuracy: 0.9875\n",
            "Epoch 10/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.1046 - accuracy: 0.9875\n",
            "Epoch 11/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0710 - accuracy: 1.0000\n",
            "Epoch 12/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0476 - accuracy: 1.0000\n",
            "Epoch 13/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0356 - accuracy: 1.0000\n",
            "Epoch 14/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0288 - accuracy: 1.0000\n",
            "Epoch 15/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0181 - accuracy: 1.0000\n",
            "Epoch 16/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0140 - accuracy: 1.0000\n",
            "Epoch 17/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0101 - accuracy: 1.0000\n",
            "Epoch 18/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0075 - accuracy: 1.0000\n",
            "Epoch 19/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - accuracy: 1.0000\n",
            "Epoch 20/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0046 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 0.8765 - accuracy: 0.8000\n",
            "acc 0.888888888888889\n",
            "Epoch 1/20\n",
            "16/16 [==============================] - 1s 2ms/step - loss: 0.6960 - accuracy: 0.5000\n",
            "Epoch 2/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6386 - accuracy: 0.7125\n",
            "Epoch 3/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.5880 - accuracy: 0.9500\n",
            "Epoch 4/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.9750\n",
            "Epoch 5/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.4625 - accuracy: 0.9750\n",
            "Epoch 6/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.4062 - accuracy: 0.9875\n",
            "Epoch 7/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.3349 - accuracy: 0.9875\n",
            "Epoch 8/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2589 - accuracy: 1.0000\n",
            "Epoch 9/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2136 - accuracy: 0.9875\n",
            "Epoch 10/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.1530 - accuracy: 1.0000\n",
            "Epoch 11/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.1186 - accuracy: 0.9875\n",
            "Epoch 12/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0911 - accuracy: 1.0000\n",
            "Epoch 13/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0711 - accuracy: 1.0000\n",
            "Epoch 14/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0513 - accuracy: 1.0000\n",
            "Epoch 15/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0432 - accuracy: 1.0000\n",
            "Epoch 16/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0292 - accuracy: 1.0000\n",
            "Epoch 17/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0229 - accuracy: 1.0000\n",
            "Epoch 18/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0163 - accuracy: 1.0000\n",
            "Epoch 19/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0144 - accuracy: 1.0000\n",
            "Epoch 20/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0112 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 0.0587 - accuracy: 1.0000\n",
            "acc 1.0\n",
            "Epoch 1/20\n",
            "16/16 [==============================] - 1s 2ms/step - loss: 0.6820 - accuracy: 0.5125\n",
            "Epoch 2/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6485 - accuracy: 0.7750\n",
            "Epoch 3/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6001 - accuracy: 0.8750\n",
            "Epoch 4/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.5502 - accuracy: 0.9125\n",
            "Epoch 5/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.4712 - accuracy: 0.9750\n",
            "Epoch 6/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.4156 - accuracy: 0.9500\n",
            "Epoch 7/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.3468 - accuracy: 0.9750\n",
            "Epoch 8/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2608 - accuracy: 0.9875\n",
            "Epoch 9/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2099 - accuracy: 0.9750\n",
            "Epoch 10/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.1600 - accuracy: 0.9875\n",
            "Epoch 11/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.1183 - accuracy: 1.0000\n",
            "Epoch 12/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0983 - accuracy: 0.9875\n",
            "Epoch 13/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0853 - accuracy: 0.9875\n",
            "Epoch 14/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0552 - accuracy: 1.0000\n",
            "Epoch 15/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0536 - accuracy: 0.9875\n",
            "Epoch 16/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0327 - accuracy: 1.0000\n",
            "Epoch 17/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 1.0000\n",
            "Epoch 18/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0221 - accuracy: 1.0000\n",
            "Epoch 19/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0143 - accuracy: 1.0000\n",
            "Epoch 20/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0113 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.0438 - accuracy: 1.0000\n",
            "acc 1.0\n",
            "Epoch 1/20\n",
            "16/16 [==============================] - 1s 2ms/step - loss: 0.7393 - accuracy: 0.5000\n",
            "Epoch 2/20\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6146 - accuracy: 0.7375\n",
            "Epoch 3/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.5587 - accuracy: 0.9500\n",
            "Epoch 4/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.5045 - accuracy: 0.9625\n",
            "Epoch 5/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.4341 - accuracy: 0.9875\n",
            "Epoch 6/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.3295 - accuracy: 1.0000\n",
            "Epoch 7/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2929 - accuracy: 1.0000\n",
            "Epoch 8/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2233 - accuracy: 1.0000\n",
            "Epoch 9/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.1868 - accuracy: 0.9875\n",
            "Epoch 10/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.1237 - accuracy: 1.0000\n",
            "Epoch 11/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0847 - accuracy: 1.0000\n",
            "Epoch 12/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0693 - accuracy: 1.0000\n",
            "Epoch 13/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0547 - accuracy: 1.0000\n",
            "Epoch 14/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0384 - accuracy: 1.0000\n",
            "Epoch 15/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0281 - accuracy: 1.0000\n",
            "Epoch 16/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0279 - accuracy: 1.0000\n",
            "Epoch 17/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0203 - accuracy: 1.0000\n",
            "Epoch 18/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0149 - accuracy: 1.0000\n",
            "Epoch 19/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0105 - accuracy: 1.0000\n",
            "Epoch 20/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0082 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 0.0208 - accuracy: 1.0000\n",
            "acc 1.0\n",
            "Epoch 1/20\n",
            "16/16 [==============================] - 1s 2ms/step - loss: 0.6768 - accuracy: 0.5375\n",
            "Epoch 2/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6325 - accuracy: 0.7875\n",
            "Epoch 3/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.5664 - accuracy: 0.9625\n",
            "Epoch 4/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.4993 - accuracy: 0.9750\n",
            "Epoch 5/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.3963 - accuracy: 0.9750\n",
            "Epoch 6/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.3117 - accuracy: 0.9750\n",
            "Epoch 7/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2360 - accuracy: 0.9875\n",
            "Epoch 8/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.1718 - accuracy: 1.0000\n",
            "Epoch 9/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.1220 - accuracy: 1.0000\n",
            "Epoch 10/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0942 - accuracy: 1.0000\n",
            "Epoch 11/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0647 - accuracy: 1.0000\n",
            "Epoch 12/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0466 - accuracy: 1.0000\n",
            "Epoch 13/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0345 - accuracy: 1.0000\n",
            "Epoch 14/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0217 - accuracy: 1.0000\n",
            "Epoch 15/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0180 - accuracy: 1.0000\n",
            "Epoch 16/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0123 - accuracy: 1.0000\n",
            "Epoch 17/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0112 - accuracy: 1.0000\n",
            "Epoch 18/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0075 - accuracy: 1.0000\n",
            "Epoch 19/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0052 - accuracy: 1.0000\n",
            "Epoch 20/20\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0052 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 0.0114 - accuracy: 1.0000\n",
            "acc 1.0\n",
            "accuracy 0.9800000011920929\n",
            "f1 0.9888888888888889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LsMNhETEOYQ",
        "outputId": "e97d4eca-d15b-42dd-9d52-d6eda2c816c2"
      },
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        " \n",
        "auc=[]\n",
        "accu=[]\n",
        "f1=[]\n",
        "\n",
        "X=large_x#x_covid##newC#newMat#x_data\n",
        "y=large_y\n",
        "input_shape=X.shape[1]\n",
        "\n",
        "sf=StratifiedKFold(n_splits=5, random_state=1, shuffle=True)\n",
        "\n",
        "for train_ix , test_ix in sf.split(X, y):\n",
        "\n",
        "  \n",
        "  model = Sequential()\n",
        "  model.add(Dense(128, input_dim=input_shape, activation='sigmoid'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(128, activation='sigmoid'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(64, activation='sigmoid'))\n",
        "  model.add(Dense(3,activation='softmax'))\n",
        " \n",
        "  model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
        "\n",
        "  train_x,test_x = X[train_ix],X[test_ix]\n",
        "  train_y,test_y = y[train_ix],y[test_ix]\n",
        "  train_y= np_utils.to_categorical(train_y,3)\n",
        "  test_y = np_utils.to_categorical(test_y,3)\n",
        "\n",
        "  oversample=SMOTE()\n",
        "  train_x,train_y=oversample.fit_resample(train_x, train_y)\n",
        "  model.fit(train_x,train_y,epochs=40,batch_size=10,verbose=0)\n",
        "  y_pred=model.predict(test_x,batch_size=10)\n",
        "  y_pred-np.asarray(y_pred)\n",
        "\n",
        "  score = model.evaluate(test_x,test_y, batch_size=20)\n",
        "  auc.append(score[1])\n",
        "  f1=f1_score(test_y,y_pred.round(),average='micro')\n",
        "  print('acc',f1)\n",
        "  accu.append(f1)\n",
        "\n",
        "auc=mean(auc)\n",
        "accu=np.asarray(accu)\n",
        "accu=mean(accu)\n",
        "\n",
        "remain_data_y_cate= np_utils.to_categorical(remain_data_y,3)\n",
        "score_test=model.evaluate(remain_data,remain_data_y_cate,batch_size=10)\n",
        "print(\"test_score:loss&accuracy\",score_test)\n",
        "\n",
        "print('accuracy',auc)\n",
        "print('f1',accu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 0s 3ms/step - loss: 5.6522e-07 - accuracy: 1.0000\n",
            "acc 1.0\n",
            "3/3 [==============================] - 0s 3ms/step - loss: 2.3225e-07 - accuracy: 1.0000\n",
            "acc 1.0\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "acc 1.0\n",
            "3/3 [==============================] - 0s 3ms/step - loss: 1.6041e-06 - accuracy: 1.0000\n",
            "acc 1.0\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.0323 - accuracy: 1.0000\n",
            "acc 1.0\n",
            "3/3 [==============================] - 0s 3ms/step - loss: 2.5059 - accuracy: 0.7667\n",
            "test_score:loss&accuracy [2.50594425201416, 0.7666666507720947]\n",
            "accuracy 1.0\n",
            "f1 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJVgpS_CbGCB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02d7ed5e-6a85-43b3-a6e5-24a6dbffa601"
      },
      "source": [
        "#MLP for covid\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "auc=[]\n",
        "accu=[]\n",
        "f1=[]\n",
        "\n",
        "X=newMat_RNA#newMat#newC#newMat#x_data\n",
        "y=y_label_RNA#y_label\n",
        "input_shape=X.shape[1]\n",
        "\n",
        "sf=StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\n",
        "\n",
        "for train_ix , test_ix in sf.split(X, y):\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Dense(128, input_dim=input_shape, activation='sigmoid'))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(128, activation='sigmoid'))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(64, activation='sigmoid'))\n",
        "  model.add(Dense(1,activation='sigmoid'))\n",
        " \n",
        "  model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
        "\n",
        "  train_x,test_x = X[train_ix],X[test_ix]\n",
        "  train_y,test_y = np.array(y)[train_ix],np.array(y)[test_ix]\n",
        "\n",
        "\n",
        "  oversample=SMOTE()\n",
        "  train_x,train_y=oversample.fit_resample(train_x, train_y)\n",
        "  model.fit(train_x,train_y,epochs=30,batch_size=10,verbose=0)\n",
        "\n",
        "\n",
        "  score = model.evaluate(test_x,test_y, batch_size=20)\n",
        "\n",
        "  y_pred=model.predict(test_x,batch_size=20)\n",
        "\n",
        "  y_pred-np.asarray(y_pred)\n",
        "  #print(test_y,y_pred)\n",
        "\n",
        "  accuracy=accuracy_score(test_y,y_pred.round())\n",
        "  \n",
        "  #accuracy=metrics.categorical_accuracy(test_y,y_pred)\n",
        "  f1=f1_score(test_y,y_pred.round())\n",
        "  print('acc',f1)\n",
        "  accu.append(f1)\n",
        "  auc.append(score[1])\n",
        "  \n",
        "\n",
        "auc=mean(auc)\n",
        "accu=np.asarray(accu)\n",
        "accu=mean(accu)\n",
        "\n",
        "print('accuracy',auc)\n",
        "print('f1',accu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 169ms/step - loss: 3.0534e-04 - accuracy: 1.0000\n",
            "acc 1.0\n",
            "1/1 [==============================] - 0s 140ms/step - loss: 6.1796e-04 - accuracy: 1.0000\n",
            "acc 1.0\n",
            "1/1 [==============================] - 0s 136ms/step - loss: 0.0087 - accuracy: 1.0000\n",
            "acc 1.0\n",
            "1/1 [==============================] - 0s 138ms/step - loss: 0.0027 - accuracy: 1.0000\n",
            "acc 1.0\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_test_function.<locals>.test_function at 0x7efb80167c20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 2.1787e-04 - accuracy: 1.0000\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7efb81f53950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "acc 1.0\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_test_function.<locals>.test_function at 0x7efb81f928c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 5.8665e-04 - accuracy: 1.0000\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7efb81d764d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "acc 1.0\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 3.6783e-04 - accuracy: 1.0000\n",
            "acc 1.0\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.1523 - accuracy: 0.8000\n",
            "acc 0.8571428571428571\n",
            "1/1 [==============================] - 0s 140ms/step - loss: 0.0012 - accuracy: 1.0000\n",
            "acc 1.0\n",
            "1/1 [==============================] - 0s 139ms/step - loss: 1.9243e-04 - accuracy: 1.0000\n",
            "acc 1.0\n",
            "accuracy 0.9800000011920929\n",
            "f1 0.9857142857142858\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G56fmM8jXjWD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4d5f58d-c420-4c9b-ee8b-49bc2c7cfcb3"
      },
      "source": [
        "#MLP的一个尝试\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "auc=[]\n",
        "\n",
        "X=newMat#x_data\n",
        "y=y_label\n",
        "\n",
        "sf=StratifiedKFold(n_splits=3, random_state=1, shuffle=True)\n",
        "\n",
        "for train_ix , test_ix in sf.split(X, y):\n",
        "\n",
        "  \n",
        "  train_x,test_x = X[train_ix],X[test_ix]\n",
        "  train_y,test_y = y[train_ix],y[test_ix]\n",
        "  oversample=SMOTE(k_neighbors=3)\n",
        "\n",
        "  train_x,train_y=oversample.fit_resample(train_x,train_y)\n",
        "  test_x,test_y=oversample.fit_resample(test_x,test_y)\n",
        "\n",
        "  \n",
        "  clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(30,20), random_state=1)\n",
        "  clf.fit(train_x,train_y)\n",
        "  predict_results=clf.predict(test_x)\n",
        "  score=accuracy_score(predict_results,test_y)\n",
        "  \n",
        "\n",
        "  auc.append(score)\n",
        "auc=mean(auc)\n",
        "print(auc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7401960784313726\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN"
      ],
      "metadata": {
        "id": "34mQyPukEIDg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf_UKVQP_AA-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "outputId": "f9d9c304-edcc-4681-8934-2e3ddf09966f"
      },
      "source": [
        "#CNN\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
        "\n",
        "auc=[]\n",
        "accu=[]\n",
        "f1=[]\n",
        "\n",
        "\n",
        "#X=newMat_sev#newMat#x_data\n",
        "#y=y_sev_label#y_label#\n",
        "\n",
        "\n",
        "X=test_x#newMat_sev#x_covid##newC#newMat#x_data\n",
        "y=test_y#y_sev_label\n",
        "input_shape=X.shape[1]\n",
        "\n",
        "sf=StratifiedKFold(n_splits=5, random_state=1, shuffle=True)\n",
        "\n",
        "for train_ix , test_ix in sf.split(X, y):\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Conv1D(32, 5, activation='relu', input_shape=(input_shape,1)))\n",
        "  model.add(Conv1D(64, 5, activation='relu'))\n",
        "  model.add(MaxPooling1D(3))\n",
        "  model.add(Conv1D(128, 5, activation='relu'))\n",
        "  model.add(Conv1D(128, 5, activation='relu'))\n",
        "  model.add(GlobalAveragePooling1D())\n",
        "  #model.add(Dropout(0.2))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        " \n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='SGD',\n",
        "              metrics=['accuracy'])\n",
        "  \n",
        "  train_x,test_x = X[train_ix],X[test_ix]\n",
        "  train_y,test_y = y[train_ix],y[test_ix]\n",
        "  oversample=SMOTE(k_neighbors=3)\n",
        "\n",
        "  train_x,train_y=oversample.fit_resample(train_x,train_y)\n",
        "  train_x = train_x[..., None]\n",
        "\n",
        "  test_x,test_y=oversample.fit_resample(test_x,test_y)\n",
        "  test_x = test_x[..., None]\n",
        "  \n",
        "  model.fit(train_x, train_y, batch_size=20, epochs=20)\n",
        "  score = model.evaluate(test_x,test_y, batch_size=20)\n",
        "  y_pred=model.predict(test_x,batch_size=20)\n",
        "\n",
        "  y_pred-np.asarray(y_pred)\n",
        "  f1=f1_score(test_y,y_pred.round(),average='micro')\n",
        "  print('acc',f1)\n",
        "  \n",
        "  accu.append(f1)\n",
        "  auc.append(score[1])\n",
        "\n",
        "auc=mean(auc)\n",
        "accu=np.asarray(accu)\n",
        "accu=mean(accu)\n",
        "\n",
        "print('accuracy',auc)\n",
        "print('f1',accu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-104-c91c00810aaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0moversample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSMOTE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m   \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moversample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m   \u001b[0mtrain_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/imblearn/base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     81\u001b[0m         )\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         y_ = (\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/imblearn/over_sampling/_smote/base.py\u001b[0m in \u001b[0;36m_fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn_k_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m             \u001b[0mnns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn_k_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m             X_new, y_new = self._make_samples(\n\u001b[1;32m    311\u001b[0m                 \u001b[0mX_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    724\u001b[0m             raise ValueError(\n\u001b[1;32m    725\u001b[0m                 \u001b[0;34m\"Expected n_neighbors <= n_samples, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m                 \u001b[0;34m\" but n_samples = %d, n_neighbors = %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_samples_fit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m             )\n\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected n_neighbors <= n_samples,  but n_samples = 1, n_neighbors = 4"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM"
      ],
      "metadata": {
        "id": "Kr4tc1VyE_2K"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp88zUI_jYe1"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from keras.models import Sequential, model_from_json\n",
        "from keras.layers import Dense, LSTM, Dropout\n",
        "\n",
        "class NeuralNetwork():\n",
        "    def __init__(self, **kwargs):\n",
        "        \"\"\"\n",
        "        :param **kwargs: output_dim=4: output dimension of LSTM layer; activation_lstm='tanh': activation function for LSTM layers; activation_dense='relu': activation function for Dense layer; activation_last='sigmoid': activation function for last layer; drop_out=0.2: fraction of input units to drop; np_epoch=10, the number of epoches to train the model. epoch is one forward pass and one backward pass of all the training examples; batch_size=32: number of samples per gradient update. The higher the batch size, the more memory space you'll need; loss='mean_square_error': loss function; optimizer='rmsprop'\n",
        "        \"\"\"\n",
        "        self.output_dim = kwargs.get('output_dim', 8)\n",
        "        self.activation_lstm = kwargs.get('activation_lstm', 'relu')\n",
        "        self.activation_dense = kwargs.get('activation_dense', 'relu')\n",
        "        self.activation_last = kwargs.get('activation_last', 'softmax')    # softmax for multiple output\n",
        "        self.dense_layer = kwargs.get('dense_layer', 2)     # at least 2 layers\n",
        "        self.lstm_layer = kwargs.get('lstm_layer', 2)\n",
        "        self.drop_out = kwargs.get('drop_out', 0.2)\n",
        "        self.nb_epoch = kwargs.get('nb_epoch', 10)\n",
        "        self.batch_size = kwargs.get('batch_size', 100)\n",
        "        self.loss = kwargs.get('loss', 'categorical_crossentropy')\n",
        "        self.optimizer = kwargs.get('optimizer', 'rmsprop')\n",
        "\n",
        "    def NN_model(self, trainX, trainY, testX, testY):\n",
        "        \"\"\"\n",
        "        :param trainX: training data set\n",
        "        :param trainY: expect value of training data\n",
        "        :param testX: test data set\n",
        "        :param testY: epect value of test data\n",
        "        :return: model after training\n",
        "        \"\"\"\n",
        "        print(\"Training model is LSTM network!\")\n",
        "        input_dim = trainX[1].shape[1]\n",
        "        output_dim = trainY.shape[1] # one-hot label\n",
        "        # print predefined parameters of current model:\n",
        "        model = Sequential()\n",
        "        # applying a LSTM layer with x dim output and y dim input. Use dropout parameter to avoid overfitting\n",
        "        model.add(LSTM(output_dim=self.output_dim,\n",
        "                       input_dim=input_dim,\n",
        "                       activation=self.activation_lstm,\n",
        "                       dropout_U=self.drop_out,\n",
        "                       return_sequences=True))\n",
        "        for i in range(self.lstm_layer-2):\n",
        "            model.add(LSTM(output_dim=self.output_dim,\n",
        "                       input_dim=self.output_dim,\n",
        "                       activation=self.activation_lstm,\n",
        "                       dropout_U=self.drop_out,\n",
        "                       return_sequences=True))\n",
        "        # argument return_sequences should be false in last lstm layer to avoid input dimension incompatibility with dense layer\n",
        "        model.add(LSTM(output_dim=self.output_dim,\n",
        "                       input_dim=self.output_dim,\n",
        "                       activation=self.activation_lstm,\n",
        "                       dropout_U=self.drop_out))\n",
        "        for i in range(self.dense_layer-1):\n",
        "            model.add(Dense(output_dim=self.output_dim,\n",
        "                        activation=self.activation_last))\n",
        "        model.add(Dense(output_dim=output_dim,\n",
        "                        input_dim=self.output_dim,\n",
        "                        activation=self.activation_last))\n",
        "        # configure the learning process\n",
        "        model.compile(loss=self.loss, optimizer=self.optimizer, metrics=['accuracy'])\n",
        "        # train the model with fixed number of epoches\n",
        "        model.fit(x=trainX, y=trainY, nb_epoch=self.nb_epoch, batch_size=self.batch_size, validation_data=(testX, testY))\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NN=NeuralNetwork()\n",
        "NN.NN_model(x,y,x,y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "oKTTgLxoEc43",
        "outputId": "fc21f94e-f25f-4776-b7bf-8b63eb7a12dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-c9ebc7443660>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNN_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7twVTpi_lNiY"
      },
      "source": [
        "#https://www.cnblogs.com/arkenstone/p/5794063.html\n",
        "#https://github.com/CasiaFan/time_seires_prediction_using_lstm/blob/master/neural_network_run.py\n",
        "#https://github.com/umeshpalai/AlgorithmicTrading-MachineLearning/blob/master/RNN_LSTM_GRU.py\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN"
      ],
      "metadata": {
        "id": "jWRKchEYkJgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n"
      ],
      "metadata": {
        "id": "gqD7jgmzkVs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timesteps = 100\n",
        "input_features = 32\n",
        "output_features = 64\n",
        "\n",
        "# 输入有100个时间点，每个时间点有32维的数据\n",
        "inputs = np.random.random((timesteps,input_features))\n",
        "state_t = np.zeros((output_features,))\n",
        "\n",
        "W = np.random.random((output_features,input_features)) # input的权重\n",
        "U = np.random.random((output_features,output_features)) # state的权重\n",
        "b = np.random.random((output_features,)) # bias\n",
        "\n",
        "successive_outputs = []\n",
        "for input_t in inputs:\n",
        "    # 按timesteps进行迭代\n",
        "    # output_t是一个64维的向量\n",
        "    output_t = np.tanh(np.dot(W,input_t)+np.dot(U,state_t)+b)\n",
        "    # 将当前时刻的输出保存到successive_outputs中\n",
        "    successive_outputs.append(output_t)\n",
        "    # 当前时刻的输出作为下一时刻的state\n",
        "    state_t = output_t\n",
        "    \n",
        "final_output_sequence = np.concatenate(successive_outputs,axis=0)\n"
      ],
      "metadata": {
        "id": "9ui826rEkMEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding,SimpleRNN\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(10000,32))\n",
        "model.add(SimpleRNN(32,return_sequences=True))\n",
        "model.add(SimpleRNN(32,return_sequences=True))\n",
        "model.add(SimpleRNN(32,return_sequences=True))\n",
        "model.add(SimpleRNN(32))\n"
      ],
      "metadata": {
        "id": "oSMqR2g4kSCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resnet"
      ],
      "metadata": {
        "id": "a0Gu4Qe3FGte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Xc8fFZ-WFZoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# models for N/P,sev"
      ],
      "metadata": {
        "id": "LkqNrwP-Dp8D"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1dj7KdIjRac"
      },
      "source": [
        "#Models for training\n",
        "def svm_model(X, y, sf, prediction_dict,n_splits,C_dict):\n",
        "\tauc=[]\n",
        "\taccu=[]\n",
        "\tf1=[]\n",
        "\tC=np.zeros((2,2))\n",
        " \n",
        "\tfor train_ix , test_ix in sf.split(X, y):\n",
        "\t\ttrain_x, test_x = X[train_ix], X[test_ix]\n",
        "\t\ttrain_y,test_y = np.array(y)[train_ix],np.array(y)[test_ix]\n",
        "\t\tclf=svm.SVC()\n",
        "\t\toversample=SMOTE()\n",
        "\t\ttrain_x,train_y=oversample.fit_resample(train_x, train_y)\n",
        "\t\tclf.fit(train_x, train_y)\n",
        "\t\tpredicted=clf.predict(test_x)\n",
        "\t\n",
        "\t\tC+=confusion_matrix(test_y,predicted)\n",
        "\t\tauc.append(roc_auc_score(test_y, predicted))\n",
        "\t\taccu.append(accuracy_score(test_y, predicted))\n",
        "\t\tf1.append(f1_score(test_y,predicted))\n",
        "\t\n",
        "\tauc=mean(auc)\n",
        "\taccu=mean(accu)\n",
        "\tf1=mean(f1)\n",
        "  \n",
        "\tprediction_dict[\"SVM\"]=[auc, accu, f1 ]\n",
        "\tC_dict[\"SVM\"]=C/n_splits\n",
        "\n",
        "\treturn prediction_dict,C_dict\n",
        "\n",
        "def RandomForest_model(X, y, sf, prediction_dict,n_splits,C_dict):\n",
        "\tauc=[]\n",
        "\taccu=[]\n",
        "\tf1=[]\n",
        "\tC=np.zeros((2,2))\n",
        "\tfor train_ix , test_ix in sf.split(X, y):\n",
        "\t\ttrain_x, test_x = X[train_ix], X[test_ix]\n",
        "\t\ttrain_y,test_y = np.array(y)[train_ix],np.array(y)[test_ix]\n",
        "\t\toversample=SMOTE()\n",
        "\t\ttrain_x,train_y=oversample.fit_resample(train_x, train_y)\n",
        "\t\tclf = RandomForestClassifier().fit(train_x, train_y)\n",
        "\t\tpredicted=clf.predict(test_x)\n",
        "\t\tC+=confusion_matrix(test_y,predicted)\n",
        "\t\tauc.append(roc_auc_score(test_y, predicted))\n",
        "\t\taccu.append(accuracy_score(test_y, predicted))\n",
        "\t\tf1.append(f1_score(test_y,predicted))\n",
        "\t\n",
        "\tauc=mean(auc)\n",
        "\taccu=mean(accu)\n",
        "\tf1=mean(f1)\n",
        "  \n",
        "\tprediction_dict[\"RandomForest\"]=[auc, accu, f1 ]\n",
        "\n",
        "\tC_dict[\"RandomForest\"]=C/n_splits\n",
        "\n",
        "\treturn prediction_dict,C_dict\n",
        "\n",
        "def Logistic_model(X, y, sf,prediction_dict,n_splits,C_dict):\n",
        "\n",
        "\tauc=[]\n",
        "\taccu=[]\n",
        "\tf1=[]\n",
        "\tC=np.zeros((2,2))\n",
        "\tfor train_ix , test_ix in sf.split(X, y):\n",
        "\t\ttrain_x, test_x = X[train_ix], X[test_ix]\n",
        "\t\ttrain_y,test_y = np.array(y)[train_ix],np.array(y)[test_ix]\n",
        "\t\toversample=SMOTE()\n",
        "\t\ttrain_x,train_y=oversample.fit_resample(train_x, train_y)\n",
        "\t\tclf = LogisticRegression().fit(train_x, train_y)\n",
        "\t\tpredicted=clf.predict(test_x)\n",
        "\t\tC+=confusion_matrix(test_y,predicted)\n",
        "\t\tauc.append(roc_auc_score(test_y, predicted))\n",
        "\t\taccu.append(accuracy_score(test_y, predicted))\n",
        "\t\tf1.append(f1_score(test_y,predicted))\n",
        "\t\n",
        "\tauc=mean(auc)\n",
        "\taccu=mean(accu)\n",
        "\tf1=mean(f1)\n",
        "  \n",
        "\tprediction_dict[\"Logistic Regression\"]=[auc, accu, f1 ]\n",
        "\tC_dict[\"Logistic Regression\"]=C/n_splits\n",
        "\treturn prediction_dict,C_dict\n",
        "\n",
        "def Knearest_model(X, y, sf ,prediction_dict,n_splits,C_dict):\n",
        "\n",
        "\tauc=[]\n",
        "\taccu=[]\n",
        "\tf1=[]\n",
        "\tC=np.zeros((2,2))\n",
        "\tfor train_ix , test_ix in sf.split(X, y):\n",
        "\t\ttrain_x, test_x = X[train_ix], X[test_ix]\n",
        "\t\ttrain_y,test_y = np.array(y)[train_ix],np.array(y)[test_ix]\n",
        "\t\t\n",
        "\t\toversample=SMOTE()\n",
        "\t\ttrain_x,train_y=oversample.fit_resample(train_x, train_y)\n",
        "\t\tclf = KNeighborsClassifier().fit(train_x, train_y)\n",
        "\t\tpredicted=clf.predict(test_x)\n",
        "\t\tC+=confusion_matrix(test_y,predicted)\n",
        "\t\tauc.append(roc_auc_score(test_y, predicted))\n",
        "\t\taccu.append(accuracy_score(test_y, predicted))\n",
        "\t\tf1.append(f1_score(test_y,predicted))\n",
        "\t\n",
        "\tauc=mean(auc)\n",
        "\taccu=mean(accu)\n",
        "\tf1=mean(f1)\n",
        "\tprediction_dict[\"Knearest Model\"]=[auc, accu, f1 ]\n",
        "\tC_dict[\"Knearest Model\"]=C/n_splits\n",
        "\n",
        "\treturn prediction_dict,C_dict\n",
        "\n",
        "def BalancedBagging_model(X, y,sf,prediction_dict):\n",
        "\n",
        "\tauc=[]\n",
        "\taccu=[]\n",
        "\tf1=[]\n",
        "\t\n",
        "\tfor train_ix , test_ix in sf.split(X, y):\n",
        "\t\ttrain_x, test_x = X[train_ix], X[test_ix]\n",
        "\t\ttrain_y,test_y = np.array(y)[train_ix],np.array(y)[test_ix]\n",
        "\t\toversample=SMOTE()\n",
        "\t\ttrain_x,train_y=oversample.fit_resample(train_x, train_y)\n",
        "\t\tclf = BalancedBaggingClassifier().fit(train_x, train_y)\n",
        "\t\tpredicted=clf.predict(test_x)\n",
        "\t\tauc.append(roc_auc_score(test_y, predicted))\n",
        "\t\taccu.append(accuracy_score(test_y, predicted))\n",
        "\t\tf1.append(f1_score(test_y,predicted))\n",
        "\tauc=mean(auc)\n",
        "\taccu=mean(accu)\n",
        "\tf1=mean(f1)\n",
        "\tprediction_dict[\"Balanced Bagging\"]=[ auc, accu, f1 ]\n",
        "\treturn prediction_dict\n",
        "\t\n",
        "\n",
        "def RUS_model(X, y, sf ,prediction_dict,n_splits,C_dict):\n",
        "\n",
        "\tauc=[]\n",
        "\taccu=[]\n",
        "\tf1=[]\n",
        "\tC=np.zeros((2,2))\n",
        " \n",
        "  \n",
        "\tfor train_ix , test_ix in sf.split(X, y):\n",
        "\t\ttrain_x, test_x = X[train_ix], X[test_ix]\n",
        "\t\ttrain_y,test_y = np.array(y)[train_ix],np.array(y)[test_ix]\n",
        "\t\toversample=SMOTE()\n",
        "\t\ttrain_x,train_y=oversample.fit_resample(train_x, train_y)\n",
        "\t\tclf = RUSBoostClassifier().fit(train_x, train_y)\n",
        "\t\tpredicted=clf.predict(test_x)\n",
        "\t\n",
        "\t\tC+=confusion_matrix(test_y,predicted)\n",
        "\t\tauc.append(roc_auc_score(test_y, predicted))\n",
        "\t\taccu.append(accuracy_score(test_y, predicted))\n",
        "\t\tf1.append(f1_score(test_y,predicted))\n",
        "\t\n",
        "\tauc=mean(auc)\n",
        "\taccu=mean(accu)\n",
        "\tf1=mean(f1)\n",
        "  \n",
        "\tprediction_dict[\"RUS\"]=[auc, accu, f1 ]\n",
        "\tC_dict[\"RUS\"]=C/n_splits\n",
        "\n",
        "\treturn prediction_dict,C_dict\n",
        "\n",
        "def Training(all_features, all_labels):\n",
        "\tprediction_dict={}\n",
        "\tC_dict={}\n",
        "\n",
        "\tn_splits_num=5\n",
        "\tsf=StratifiedKFold(n_splits=n_splits_num, random_state=1, shuffle=True)\n",
        " \n",
        "\tprediction_dict,C_dict=svm_model(all_features, all_labels, sf, prediction_dict,n_splits_num,C_dict)\n",
        "\tprediction_dict,C_dict=RandomForest_model(all_features, all_labels, sf, prediction_dict,n_splits_num,C_dict)\n",
        "\tprediction_dict,C_dict=Logistic_model(all_features, all_labels,sf,prediction_dict,n_splits_num,C_dict)\n",
        "\tprediction_dict,C_dict=Knearest_model(all_features, all_labels, sf, prediction_dict,n_splits_num,C_dict)\n",
        "\t#prediction_dict=BalancedBagging_model(all_features, all_labels,sf, prediction_dict)\n",
        "\tprediction_dict,C_dict=RUS_model(all_features, all_labels,sf, prediction_dict,n_splits_num,C_dict)\n",
        "\n",
        "\tl=prediction_dict.items()\n",
        "\tmaximum=0\n",
        "\tmodel=\"\"\n",
        "\tfor item in l:\n",
        "\t\tif item[1][2]>maximum:\n",
        "\t\t\tmaximum=item[1][2]\n",
        "\t\t\tmodel=item[0]\n",
        "\tprint( \"\\n--- Training and Testing Completed ! ---\\n\")\n",
        "\tprint(\"\\n\\nSelected Model: \"+ model + \" F1 score: \" + str(maximum))\n",
        "\treturn prediction_dict,C_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGbVf91P6NOq"
      },
      "source": [
        "#Models for training\n",
        "def svm_model2(X, y, sf, prediction_dict):\n",
        "\tauc=[]\n",
        "\taccu=[]\n",
        "\tf1=[]\n",
        "\tfor train_ix , test_ix in sf.split(X, y):\n",
        "\t\ttrain_x, test_x = X[train_ix], X[test_ix]\n",
        "\t\ttrain_y, test_y = y[train_ix], y[test_ix]\n",
        "\n",
        "\n",
        "\t\tclf=svm.SVC()\n",
        "\t\t#oversample=SMOTE()\n",
        "\t\t#train_x,train_y=oversample.fit_resample(train_x, train_y)\n",
        "\t\tclf.fit(train_x, train_y)\n",
        "\t\tpredicted=clf.predict(test_x)\n",
        "\t\t#auc.append(roc_auc_score(test_y, predicted,multi_class='ovo'))\n",
        "\t\taccu.append(accuracy_score(test_y, predicted))\n",
        "\t\tf1.append(f1_score(test_y,predicted,average='macro'))\n",
        "\t#auc=mean(auc)\n",
        "\taccu=mean(accu)\n",
        "\tf1=mean(f1)\n",
        "\tprediction_dict[\"SVM\"]=[auc, accu, f1 ]\n",
        "\treturn prediction_dict\n",
        "\n",
        "def RandomForest_model2(X, y, sf, prediction_dict):\n",
        "\n",
        "\tauc=[]\n",
        "\taccu=[]\n",
        "\tf1=[]\n",
        "\tfor train_ix , test_ix in sf.split(X, y):\n",
        "\t\ttrain_x, test_x = X[train_ix], X[test_ix]\n",
        "\t\ttrain_y, test_y = y[train_ix], y[test_ix]\n",
        "\n",
        "\n",
        "\t\t#oversample=SMOTE()\n",
        "\t\t#train_x,train_y=oversample.fit_resample(train_x, train_y)\n",
        "\t\tclf = RandomForestClassifier().fit(train_x, train_y)\n",
        "\t\tpredicted=clf.predict(test_x)\n",
        "\t\t#auc.append(roc_auc_score(test_y, predicted,multi_class='ovr'))\n",
        "\t\taccu.append(accuracy_score(test_y, predicted))\n",
        "\t\tf1.append(f1_score(test_y,predicted,average='macro'))\n",
        "\t#auc=mean(auc)\n",
        "\taccu=mean(accu)\n",
        "\tf1=mean(f1)\n",
        "\tprediction_dict[\"RandomForest\"]=[auc, accu, f1 ]\n",
        "\treturn prediction_dict\n",
        "\n",
        "def Logistic_model2(X, y, sf,prediction_dict):\n",
        "\n",
        "\tauc=[]\n",
        "\taccu=[]\n",
        "\tf1=[]\n",
        "\tfor train_ix , test_ix in sf.split(X, y):\n",
        "\t\ttrain_x, test_x = X[train_ix], X[test_ix]\n",
        "\t\ttrain_y, test_y = y[train_ix], y[test_ix]\n",
        "\t\t#oversample=SMOTE()\n",
        "\t\t#train_x,train_y=oversample.fit_resample(train_x, train_y)\n",
        "\t\tclf = LogisticRegression().fit(train_x, train_y)\n",
        "\t\tpredicted=clf.predict(test_x)\n",
        "\t\t#auc.append(roc_auc_score(test_y, predicted))\n",
        "\t\taccu.append(accuracy_score(test_y, predicted))\n",
        "\t\tf1.append(f1_score(test_y,predicted,average='macro'))\n",
        "\t#auc=mean(auc)\n",
        "\taccu=mean(accu)\n",
        "\tf1=mean(f1)\n",
        "\tprediction_dict[\"Logistic Regression\"]=[auc, accu, f1 ]\n",
        "\treturn prediction_dict\n",
        "def Knearest_model2(X, y, sf ,prediction_dict):\n",
        "\n",
        "\tauc=[]\n",
        "\taccu=[]\n",
        "\tf1=[]\n",
        "\tfor train_ix , test_ix in sf.split(X, y):\n",
        "\t\ttrain_x, test_x = X[train_ix], X[test_ix]\n",
        "\t\ttrain_y, test_y = y[train_ix], y[test_ix]\n",
        "\n",
        "\n",
        "\n",
        "\t\t#oversample=SMOTE()\n",
        "\t\t#train_x,train_y=oversample.fit_resample(train_x, train_y)\n",
        "\t\tclf = KNeighborsClassifier().fit(train_x, train_y)\n",
        "\t\tpredicted=clf.predict(test_x)\n",
        "\t\t#auc.append(roc_auc_score(test_y, predicted))\n",
        "\t\taccu.append(accuracy_score(test_y, predicted))\n",
        "\t\tf1.append(f1_score(test_y,predicted,average='macro'))\n",
        "\t#auc=mean(auc)\n",
        "\taccu=mean(accu)\n",
        "\tf1=mean(f1)\n",
        "\tprediction_dict[\"Knearest Model\"]=[ auc, accu, f1 ]\n",
        "\treturn prediction_dict\n",
        "\n",
        "def BalancedBagging_model2(X, y,sf,prediction_dict):\n",
        "\n",
        "\tauc=[]\n",
        "\taccu=[]\n",
        "\tf1=[]\n",
        "\tfor train_ix , test_ix in sf.split(X, y):\n",
        "\t\ttrain_x, test_x = X[train_ix], X[test_ix]\n",
        "\t\ttrain_y, test_y = y[train_ix], y[test_ix]\n",
        "\n",
        "\n",
        "\t\t#oversample=SMOTE()\n",
        "\t\t#train_x,train_y=oversample.fit_resample(train_x, train_y)\n",
        "\t\tclf = BalancedBaggingClassifier().fit(train_x, train_y)\n",
        "\t\tpredicted=clf.predict(test_x)\n",
        "\t\t#auc.append(roc_auc_score(test_y, predicted))\n",
        "\t\taccu.append(accuracy_score(test_y, predicted))\n",
        "\t\tf1.append(f1_score(test_y,predicted,average='macro'))\n",
        "\t#auc=mean(auc)\n",
        "\taccu=mean(accu)\n",
        "\tf1=mean(f1)\n",
        "\tprediction_dict[\"Balanced Bagging\"]=[ auc, accu, f1 ]\n",
        "\treturn prediction_dict\n",
        "\n",
        "def RUS_model2(X, y, sf ,prediction_dict):\n",
        "\n",
        "\tauc=[]\n",
        "\taccu=[]\n",
        "\tf1=[]\n",
        "\tfor train_ix , test_ix in sf.split(X, y):\n",
        "\t\ttrain_x, test_x = X[train_ix], X[test_ix]\n",
        "\t\ttrain_y, test_y = y[train_ix], y[test_ix]\n",
        "\n",
        "\n",
        "\t\t#oversample=SMOTE()\n",
        "\t\t#train_x,train_y=oversample.fit_resample(train_x, train_y)\n",
        "\t\tclf = RUSBoostClassifier().fit(train_x, train_y)\n",
        "\t\tpredicted=clf.predict(test_x)\n",
        "\t\t#auc.append(roc_auc_score(test_y, predicted))\n",
        "\t\taccu.append(accuracy_score(test_y, predicted))\n",
        "\t\tf1.append(f1_score(test_y,predicted,average='macro'))\n",
        "\t#auc=mean(auc)\n",
        "\taccu=mean(accu)\n",
        "\tf1=mean(f1)\n",
        "\tprediction_dict[\"RUS\"]=[auc, accu, f1 ]\n",
        "\treturn prediction_dict\n",
        "\n",
        "def Training2(all_features, all_labels):\n",
        "\tprediction_dict={}\n",
        "\tsf=StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\n",
        "\tprediction_dict=svm_model2(all_features, all_labels, sf, prediction_dict)\n",
        "\tprediction_dict=RandomForest_model2(all_features, all_labels, sf, prediction_dict)\n",
        "\tprediction_dict=Logistic_model2(all_features, all_labels,sf,prediction_dict)\n",
        "\tprediction_dict=Knearest_model2(all_features, all_labels, sf, prediction_dict)\n",
        "\t#prediction_dict=BalancedBagging_model2(all_features, all_labels,sf, prediction_dict)\n",
        "\tprediction_dict=RUS_model2(all_features, all_labels,sf, prediction_dict)\n",
        "\n",
        "\tl=prediction_dict.items()\n",
        "\tmaximum=0\n",
        "\tmodel=\"\"\n",
        "\tfor item in l:\n",
        "\t\tif item[1][2]>maximum:\n",
        "\t\t\tmaximum=item[1][2]\n",
        "\t\t\tmodel=item[0]\n",
        "\tprint( \"\\n--- Training and Testing Completed ! ---\\n\")\n",
        "\tprint(\"\\n\\nSelected Model: \"+ model + \" F1 score: \" + str(maximum))\n",
        "\treturn prediction_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train model"
      ],
      "metadata": {
        "id": "_vpcX7UxD-6e"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnYkGkyvp98Y",
        "outputId": "13353243-d33b-43c9-c655-b04ee552d1cf"
      },
      "source": [
        "X=newMat_RNA#x_data#newMat#test_x#newMat_sev#x_covid##newC#newMat#\n",
        "y=y_label_RNA#test_y#y_sev_label\n",
        "scores_dict,CM_dict=Training(X,y)\n",
        "print(scores_dict)\n",
        "print(CM_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training and Testing Completed ! ---\n",
            "\n",
            "\n",
            "\n",
            "Selected Model: SVM F1 score: 0.9894736842105263\n",
            "{'SVM': [0.95, 0.9818181818181818, 0.9894736842105263], 'RandomForest': [0.75, 0.9072727272727272, 0.9471826625386998], 'Logistic Regression': [0.95, 0.9818181818181818, 0.9894736842105263], 'Knearest Model': [0.8736111111111111, 0.7945454545454546, 0.8481738849385908], 'RUS': [0.9388888888888889, 0.9636363636363636, 0.9777089783281734]}\n",
            "{'SVM': array([[1.8, 0.2],\n",
            "       [0. , 8.8]]), 'RandomForest': array([[1. , 1. ],\n",
            "       [0. , 8.8]]), 'Logistic Regression': array([[1.8, 0.2],\n",
            "       [0. , 8.8]]), 'Knearest Model': array([[2. , 0. ],\n",
            "       [2.2, 6.6]]), 'RUS': array([[1.8, 0.2],\n",
            "       [0.2, 8.6]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "newMat_sev.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wz7rC-wj1qJE",
        "outputId": "f80f6073-03d9-464f-88a3-8b12b5d03802"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(102, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_sev_label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Rdc0Seo1xZG",
        "outputId": "1d29daf9-23f2-4244-8a69-237ffafbbab9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 2, 0, 0, 2, 0, 2, 2, 0, 0, 2, 2, 0, 0, 2, 0, 1, 0, 2,\n",
              "       0, 0, 0, 0, 2, 0, 2, 2, 2, 0, 2, 0, 2, 2, 0, 0, 0, 0, 2, 0, 0, 2,\n",
              "       2, 0, 2, 0, 0, 2, 2, 2, 2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 0, 2, 0, 0,\n",
              "       0, 2, 0, 0, 0, 2, 2, 0, 0, 0, 0, 2, 0, 2, 0, 2, 2, 1, 0, 0, 2, 0,\n",
              "       2, 2, 2, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TP4Rimx10P6",
        "outputId": "b311382c-ce45-4eb5-df2f-5c6bd17f3bdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 2, 0, 0, 2, 0, 2, 2, 0, 0, 2, 2, 0, 0, 2, 0, 1, 0, 2,\n",
              "       0, 0, 0, 0, 2, 0, 2, 2, 2, 0, 2, 0, 2, 2, 0, 0, 0, 0, 2, 0, 0, 2,\n",
              "       2, 0, 2, 0, 0, 2, 2, 2, 2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 0, 2, 0, 0,\n",
              "       0, 2, 0, 0, 0, 2, 2, 0, 0, 0, 0, 2, 0, 2, 0, 2, 2, 1, 0, 0, 2, 0,\n",
              "       2, 2, 2, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 2, 0, 0, 2,\n",
              "       0, 2, 2, 0, 0, 2, 2, 0, 0, 2, 0, 1, 0, 2, 0, 0, 0, 0, 2, 0, 2, 2,\n",
              "       2, 0, 2, 0, 2, 2, 0, 0, 0, 0, 2, 0, 0, 2, 2, 0, 2, 0, 0, 2, 2, 2,\n",
              "       2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 2, 0,\n",
              "       0, 0, 0, 2, 0, 2, 0, 2, 2, 1, 0, 0, 2, 0, 2, 2, 2, 0, 0, 0, 0, 0,\n",
              "       2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 2, 0, 0, 2, 0, 2, 2, 0, 0, 2, 2, 0,\n",
              "       0, 2, 0, 1, 0, 2, 0, 0, 0, 0, 2, 0, 2, 2, 2, 0, 2, 0, 2, 2, 0, 0,\n",
              "       0, 0, 2, 0, 0, 2, 2, 0, 2, 0, 0, 2, 2, 2, 2, 2, 0, 2, 0, 0, 2, 0,\n",
              "       0, 2, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 2, 0, 0, 0, 0, 2, 0, 2, 0, 2,\n",
              "       2, 1, 0, 0, 2, 0, 2, 2, 2, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0,\n",
              "       0, 0, 2, 0, 0, 2, 0, 2, 2, 0, 0, 2, 2, 0, 0, 2, 0, 1, 0, 2, 0, 0,\n",
              "       0, 0, 2, 0, 2, 2, 2, 0, 2, 0, 2, 2, 0, 0, 0, 0, 2, 0, 0, 2, 2, 0,\n",
              "       2, 0, 0, 2, 2, 2, 2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 0, 2, 0, 0, 0, 2,\n",
              "       0, 0, 0, 2, 2, 0, 0, 0, 0, 2, 0, 2, 0, 2, 2, 1, 0, 0, 2, 0, 2, 2,\n",
              "       2, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y69EE8RR36pa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca4fb133-1515-4bc7-a026-6307ae883e15"
      },
      "source": [
        "#different models,for severity\n",
        "#X=newMat_sev#x_covid#newMat_sev#x_data\n",
        "#y=y_sev_label\n",
        "\n",
        "X=test_x#newMat_sev#x_covid##newC#newMat#x_data\n",
        "y=test_y#y_sev_label\n",
        "scores_dict=Training2(X,y)\n",
        "print(scores_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 8 members, which is less than n_splits=10.\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 8 members, which is less than n_splits=10.\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 8 members, which is less than n_splits=10.\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 8 members, which is less than n_splits=10.\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 8 members, which is less than n_splits=10.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training and Testing Completed ! ---\n",
            "\n",
            "\n",
            "\n",
            "Selected Model: RandomForest F1 score: 1.0\n",
            "{'SVM': [[], 0.9926829268292683, 0.9949968730456535], 'RandomForest': [[], 1.0, 1.0], 'Logistic Regression': [[], 1.0, 1.0], 'Knearest Model': [[], 0.8895731707317073, 0.9151573205755651], 'RUS': [[], 0.6220731707317073, 0.703995504438282]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBGApqmpp3nu",
        "outputId": "5a005664-10bf-4ced-8112-15189ed5d077"
      },
      "source": [
        "X=x_covid#newMat_sev#x_data\n",
        "y=y_sev_label\n",
        "\n",
        "\n",
        "y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(102,)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ht5MCiGbIM2y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "4264b51a-07fe-4d83-9216-6985f1fc5c1a"
      },
      "source": [
        "#different models\n",
        "X=newMat\n",
        "y=y_label\n",
        "\n",
        "scores_dict=Training(X,y)\n",
        "print(scores_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-6c2803646a89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#different models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewMat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mscores_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'newMat' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#different models\n",
        "X=newMat_RNA\n",
        "y=y_label_RNA\n",
        "\n",
        "scores_dict=Training(X,y)\n",
        "#BalancedRandomForestClassifier\n",
        "print(scores_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-buos7ora9EA",
        "outputId": "9b54c880-342d-44d1-9d3f-d233bff6ba18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training and Testing Completed ! ---\n",
            "\n",
            "\n",
            "\n",
            "Selected Model: SVM F1 score: 0.9888888888888889\n",
            "{'SVM': [0.95, 0.98, 0.9888888888888889], 'RandomForest': [0.85, 0.9466666666666667, 0.9707070707070707], 'Logistic Regression': [0.95, 0.98, 0.9888888888888889], 'Knearest Model': [0.9325, 0.89, 0.921031746031746], 'RUS': [0.9275, 0.9433333333333334, 0.9634920634920635]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X=newMat_pm\n",
        "y=y_label_pm\n",
        "\n",
        "scores_dict=Training(X,y)\n",
        "#SVM\n",
        "print(scores_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDgu1Nihi2E-",
        "outputId": "cb053ad4-04dd-4909-9c47-86c04341d751"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training and Testing Completed ! ---\n",
            "\n",
            "\n",
            "\n",
            "Selected Model: Knearest Model F1 score: 0.5133333333333333\n",
            "{'SVM': [0.6, 0.6416666666666666, 0.26666666666666666], 'RandomForest': [0.5, 0.5583333333333333, 0.2833333333333333], 'Logistic Regression': [0.7, 0.7333333333333333, 0.5], 'Knearest Model': [0.525, 0.5083333333333333, 0.5133333333333333], 'RUS': [0.65, 0.6166666666666667, 0.41666666666666663]}\n"
          ]
        }
      ]
    }
  ]
}